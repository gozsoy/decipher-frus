{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy \n",
    "import json\n",
    "import glob\n",
    "import jellyfish\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# namespaces for xml parsing\n",
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus':'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi':'http://www.w3.org/2001/XInclude'\n",
    "      }\n",
    "\n",
    "# variables for wikidata query service\n",
    "user_agent = 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'\n",
    "sparqlwd = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "sparqlwd.setReturnFormat(JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract placeName tag within a given document and save it to city_df\n",
    "def extract_city(doc):\n",
    "\n",
    "    # city\n",
    "    place_tag = doc.find('.//dflt:placeName',ns)\n",
    "    if place_tag is not None:\n",
    "        txt = \"\".join(place_tag.itertext())\n",
    "        city = \" \".join(txt.split())\n",
    "    else:\n",
    "        city = None\n",
    "\n",
    "    global city_df\n",
    "    city_df = pd.concat((city_df, pd.DataFrame({'name':[city]})),ignore_index=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = 1952, 1988\n",
    "\n",
    "city_df = pd.DataFrame(columns=['name'])\n",
    "\n",
    "for file in glob.glob('volumes/frus*'):\n",
    "    file_start_year = int(file[12:16])\n",
    "    \n",
    "    if file_start_year >= start_year and file_start_year<=end_year:\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        docs = root.findall('./dflt:text/dflt:body//dflt:div[@type=\"document\"]', ns)\n",
    "        for doc in docs:\n",
    "            extract_city(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.dropna(inplace=True)\n",
    "city_df.drop_duplicates(inplace=True)\n",
    "city_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension_col = city_df['name'].apply(lambda x: \" \".join(x.split(',')[1:]))\n",
    "name_col = city_df['name'].apply(lambda x: x.split(',')[0])\n",
    "city_df['name'] = name_col\n",
    "city_df['extension'] = extension_col\n",
    "city_df['extension'] = city_df['extension'].apply(lambda x: None if len(x)==0 else x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wc matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using external source for quick matching before wikidata\n",
    "wc_df = pd.read_csv('tables/world-cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def geo_match(pattern,string):\n",
    "    \n",
    "    if pattern !=pattern:\n",
    "        return None\n",
    "    elif re.search(pattern,string):\n",
    "        return pattern\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# searches if extension is either country or subcountry. if so return its country\n",
    "def f(string):\n",
    "\n",
    "    if not string:\n",
    "        return None\n",
    "    \n",
    "    tl = list(wc_df[wc_df['country'].apply(lambda pattern: True if geo_match(pattern,string) else False)].drop_duplicates(subset='country')['country'].values)\n",
    "    if len(tl)==0:\n",
    "        tl = list(wc_df[wc_df['subcountry'].apply(lambda pattern: True if geo_match(pattern,string) else False)].drop_duplicates(subset='country')['country'].values)\n",
    "\n",
    "    if len(tl)==0:\n",
    "        return None\n",
    "    elif len(tl)==1:\n",
    "        return tl[0]\n",
    "    else:\n",
    "        print(f'multi-match for {string}. Check later!')\n",
    "        return tl\n",
    "\n",
    "# searches if name is subcountry. if so return its country\n",
    "def f2(string):\n",
    "\n",
    "    if not string:\n",
    "        return None\n",
    "\n",
    "    tl = list(wc_df[wc_df['subcountry'].apply(lambda pattern: True if geo_match(pattern,string) else False)].drop_duplicates(subset='country')['country'].values)\n",
    "\n",
    "    if len(tl)==0:\n",
    "        return None\n",
    "    elif len(tl)==1:\n",
    "        return tl[0]\n",
    "    else:\n",
    "        print(f'multi-match for {string}. Check later!')\n",
    "        return tl\n",
    "\n",
    "def merger(row):\n",
    "\n",
    "    d1 = row['extension_match']\n",
    "    d2 = row['wc_guess']\n",
    "\n",
    "    if not d1 and d2!=d2:\n",
    "        return None\n",
    "    elif not d1:\n",
    "        return d2\n",
    "    else:\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-match for  Maryland. Check later!\n",
      "multi-match for  Florida. Check later!\n",
      "multi-match for  Maryland. Check later!\n",
      "multi-match for  Florida. Check later!\n",
      "multi-match for  West Virginia. Check later!\n",
      "multi-match for  South Carolina. Check later!\n",
      "multi-match for  South Carolina. Check later!\n",
      "multi-match for  North Carolina. Check later!\n",
      "multi-match for  Nigeria. Check later!\n",
      "multi-match for  Maryland. Check later!\n",
      "multi-match for  Maryland. Check later!\n",
      "multi-match for  Maryland. Check later!\n",
      "multi-match for  Florida. Check later!\n",
      "multi-match for  Florida. Check later!\n",
      "multi-match for  Dominican Republic. Check later!\n",
      "multi-match for Salzburg White House. Check later!\n",
      "multi-match for Dar es Salaam. Check later!\n",
      "multi-match for Montevideo. Check later!\n",
      "multi-match for San JosÃ©. Check later!\n",
      "multi-match for San Salvador. Check later!\n",
      "multi-match for Salzburg. Check later!\n",
      "multi-match for Washington Wellington. Check later!\n",
      "multi-match for Washington Bern. Check later!\n",
      "multi-match for East Berlin. Check later!\n",
      "multi-match for La Paz. Check later!\n",
      "multi-match for Maryland. Check later!\n",
      "multi-match for West Berlin. Check later!\n",
      "multi-match for West Virginia. Check later!\n",
      "multi-match for Santiago de Cuba. Check later!\n",
      "multi-match for San Carlos de Bariloche. Check later!\n"
     ]
    }
   ],
   "source": [
    "city_df['extension_match'] = city_df['extension'].apply(lambda x:f(x))\n",
    "city_df['wc_guess'] = city_df[city_df['extension'].isna()]['name'].apply(lambda x:f2(x))\n",
    "#city_df['merged']=city_df.apply(lambda x: merger(x),axis=1) REMOVE THIS\n",
    "city_df['country']=city_df['extension_match'] # MODIFIED THIS\n",
    "\n",
    "city_df=city_df[['name','country','extension_match','wc_guess']]\n",
    "#city_df.rename(columns={'merged':'country'},inplace=True) REMOVE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and edit\n",
    "city_df.to_csv('tables/tables_52_88/city_52_88.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to csv file. Resolve multi-match cases in 'country' column by hand, and save. Proceeding next part!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikidata matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corrected one\n",
    "city_df = pd.read_csv('tables/tables_52_88/city_52_88.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_if_capital(name):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?country ?countryLabel WHERE {\n",
    "        SERVICE wikibase:mwapi {\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search  \\'\"\"\"+name+\"\"\"\\';\n",
    "                            mwapi:language \"en\".\n",
    "            ?city wikibase:apiOutputItem mwapi:item.\n",
    "            ?num wikibase:apiOrdinal true.\n",
    "        }\n",
    "        ?city wdt:P31 wd:Q5119.\n",
    "        ?city wdt:P17 ?country.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {name}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def find_if_bigcity(name):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?country ?countryLabel WHERE {\n",
    "        SERVICE wikibase:mwapi {\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search  \\'\"\"\"+name+\"\"\"\\';\n",
    "                            mwapi:language \"en\".\n",
    "            ?city wikibase:apiOutputItem mwapi:item.\n",
    "            ?num wikibase:apiOrdinal true.\n",
    "        }\n",
    "        ?city (wdt:P31/wdt:P279*) wd:Q1549591.\n",
    "        ?city wdt:P17 ?country.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {name}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_name(row, f):\n",
    "\n",
    "    name = row['name']\n",
    "\n",
    "    res = f(name)\n",
    "\n",
    "    candidates = []\n",
    "    selected_country = None\n",
    "    selected_tag = None\n",
    "\n",
    "    for binding in res['results']['bindings']:\n",
    "        candidates.append(binding['countryLabel']['value'])\n",
    "\n",
    "    if len(candidates)>0:\n",
    "        temp_country = Counter(candidates).most_common(1)[0][0]\n",
    "        temp_tag = None\n",
    "\n",
    "        for binding in res['results']['bindings']:\n",
    "            if binding['countryLabel']['value'] == temp_country:\n",
    "                temp_tag = binding['country']['value']\n",
    "                break\n",
    "        \n",
    "        #selected_country.add(temp_country)\n",
    "        selected_country = temp_country\n",
    "        #wiki_tag.add(temp_tag)\n",
    "        selected_tag = temp_tag\n",
    "\n",
    "    return selected_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find country if city is capital\n",
    "wiki_df = city_df[city_df['extension_match'].apply(lambda x: True if x!=x else False)]\n",
    "city_df['wiki_capital_guess'] = wiki_df.apply(process_name,axis=1,f=find_if_capital)\n",
    "\n",
    "# find country if city is big city but not capital\n",
    "wiki_df = city_df[city_df['extension_match'].apply(lambda x: True if x!=x else False) & city_df['wiki_capital_guess'].apply(lambda x: False if x else True)]\n",
    "city_df['wiki_bigcity_guess'] = wiki_df.apply(process_name,axis=1,f=find_if_bigcity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two columns about wikidata querying into one\n",
    "def merger2(row):\n",
    "\n",
    "    d1 = row['wiki_capital_guess']\n",
    "    d2 = row['wiki_bigcity_guess']\n",
    "\n",
    "    if (not d2 or d2!=d2) and (not d1 or d1!=d1):\n",
    "        return None\n",
    "    elif (not d2 or d2!=d2):\n",
    "        return d1\n",
    "    else:\n",
    "        return d2\n",
    "\n",
    "city_df['merged_wiki'] = city_df.apply(merger2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if country field is not filled before by dataset, use info from wikidata to fill it\n",
    "def merger3(row):\n",
    "\n",
    "    d1 = row['country']\n",
    "    d2 = row['merged_wiki']\n",
    "\n",
    "    if not d2 and d1!=d1:\n",
    "        return None\n",
    "    elif d1!=d1:\n",
    "        return d2\n",
    "    else:\n",
    "        return d1\n",
    "\n",
    "city_df['country'] = city_df.apply(merger3,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if country field is not filled before by neither dataset nor wikidata, then use wc_guess field\n",
    "# this means placeName is actually not city but a larger region\n",
    "def merger4(row):\n",
    "\n",
    "    d1 = row['country']\n",
    "    d2 = row['wc_guess']\n",
    "\n",
    "    if (not d2 or d2!=d2) and (not d1 or d1!=d1):\n",
    "        return None\n",
    "    elif (not d1 or d1!=d1):\n",
    "        return d2\n",
    "    else:\n",
    "        return d1\n",
    "\n",
    "city_df['country'] = city_df.apply(merger4,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and edit\n",
    "city_df.to_csv('tables/tables_52_88/city_52_88.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to csv file. Resolve multi-match cases in 'country' column by hand, and save. Proceeding next part!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misspelling matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corrected one\n",
    "city_df = pd.read_csv('tables/tables_52_88/city_52_88.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df=city_df[['name','country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = city_df['name'].values\n",
    "\n",
    "def compute_sim(s1,func,s2):\n",
    "    return func(s1,s2)\n",
    "\n",
    "def find_matches(s2):\n",
    "\n",
    "    spiro_dist_df = pd.DataFrame({'name_set':all_names,\n",
    "                                'dam_lev_dist':[compute_sim(x, jellyfish.damerau_levenshtein_distance,s2) for x in all_names]})\n",
    "    \n",
    "    misspelling_idx = set(spiro_dist_df[(spiro_dist_df['dam_lev_dist'] <=1)].index.values)\n",
    "\n",
    "    return misspelling_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 532/532 [00:00<00:00, 720.46it/s]\n"
     ]
    }
   ],
   "source": [
    "t = {}\n",
    "for idx in tqdm(range(len(all_names))):\n",
    "    name = all_names[idx]\n",
    "    t[idx]=find_matches(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 106 keys.\n",
      "---\n",
      "removing 2 keys.\n",
      "---\n",
      "removing 0 keys.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_key in t:\n",
    "    \n",
    "    te_df = city_df.iloc[list(t[temp_key])]\n",
    "\n",
    "    name_list = te_df['name'].values\n",
    "\n",
    "    country_list = te_df['country'].values\n",
    "    country_list = [c for c in country_list if c==c]\n",
    "    country_list = list(set(country_list))\n",
    "    if len(country_list)==0:\n",
    "        country_list = None\n",
    "    elif len(country_list)==1:\n",
    "        country_list = country_list[0]\n",
    "\n",
    "    city_df.at[temp_key, 'name_list'] = name_list\n",
    "    city_df.at[temp_key, 'country'] = country_list\n",
    "\n",
    "city_df = city_df.loc[t.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving as parquet is because to retrieve name_list column as list\n",
    "city_df.to_csv('tables/tables_52_88/city_52_88.csv')\n",
    "\n",
    "city_df[['name_list']].to_parquet('tables/tables_52_88/city_52_88_namelist.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to csv file. Resolve multi-match cases in 'country' column by hand, and save. Proceeding next part!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finalize process with several last steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corrected one\n",
    "city_df = pd.read_csv('tables/tables_52_88/city_52_88.csv')\n",
    "name_list_df = pd.read_parquet('tables/tables_52_88/city_52_88_namelist.parquet').reset_index(drop=True)\n",
    "city_df['name_list']=name_list_df['name_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is a quick remedy. \n",
    "# since number of countries for name fix is limited, can be used all times.\n",
    "def name_converter(name):\n",
    "\n",
    "    if name == 'United States':\n",
    "        return 'United States of America'\n",
    "    elif name == 'China':\n",
    "        return \"People's Republic of China\"\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "city_df['country'] = city_df['country'].apply(name_converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for matching city names with doc_df\n",
    "city_lookup_dict = {} # 'misspelled':'corrected'\n",
    "\n",
    "for _, row in city_df.iterrows():\n",
    "\n",
    "    for misspelled_name in row['name_list']:\n",
    "        \n",
    "        if misspelled_name not in city_lookup_dict:\n",
    "            city_lookup_dict[misspelled_name] = row['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df = city_df[['name','country','name_list']]\n",
    "\n",
    "city_df.to_parquet('tables/tables_52_88/city_52_88_final.parquet')\n",
    "\n",
    "json = json.dumps(city_lookup_dict)\n",
    "f = open(\"tables/tables_52_88/city_lookup_dict.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create country-wikiTag dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_country(name):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?country ?countryLabel WHERE {\n",
    "        SERVICE wikibase:mwapi {\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search  \\\"\"\"\"+name+\"\"\"\\\";\n",
    "                            mwapi:language \"en\".\n",
    "            ?country wikibase:apiOutputItem mwapi:item.\n",
    "            ?num wikibase:apiOrdinal true.\n",
    "        }\n",
    "        ?country wdt:P31 wd:Q6256.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {name}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "    \n",
    "    \n",
    "def process_name2(name):\n",
    "\n",
    "    res = find_country(name)\n",
    "\n",
    "    if len(res['results']['bindings'])>0:\n",
    "        binding = res['results']['bindings'][0]\n",
    "\n",
    "        country = binding['countryLabel']['value']\n",
    "        tag = binding['country']['value']\n",
    "\n",
    "        return country, tag\n",
    "    \n",
    "    else:\n",
    "        return name, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: nan\n",
      "error message: can only concatenate str (not \"float\") to str\n"
     ]
    }
   ],
   "source": [
    "country_tag_pairs = list(map(process_name2,city_df['country'].unique()))\n",
    "\n",
    "countryLabel = list(map(lambda x:x[0],country_tag_pairs))\n",
    "countryTag = list(map(lambda x:x[1],country_tag_pairs))\n",
    "\n",
    "country_df = pd.DataFrame.from_dict({'countryLabel':countryLabel,'countryTag':countryTag})\n",
    "#remove nan entries before saving!\n",
    "country_df.to_csv('tables/tables_52_88/country_52_88.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
