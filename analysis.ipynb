{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "import sqlite3\n",
    "import sqllite_handler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('tables/texts_69_76.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "res = cur.execute(\"SELECT TEXT FROM transcript\")\n",
    "fetched = res.fetchall()\n",
    "free_text_list = list(map(lambda x: x[0], fetched))\n",
    "\n",
    "topic_model = BERTopic.load(\"plots/topic_model_69_76\")\n",
    "\n",
    "doc_df = pd.read_csv('tables/doc_69_76.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_doc_topic_df = topic_model.get_document_info(free_text_list)\n",
    "\n",
    "topic_desc_df = messy_doc_topic_df[['Name','Top_n_words']].drop_duplicates(ignore_index=True)\n",
    "doc_topic_df = pd.DataFrame({'id_to_text':doc_df['id_to_text'],'assigned_topic':messy_doc_topic_df['Name']})\n",
    "\n",
    "topic_desc_df.to_csv('tables/topic_descp_69_76.csv')\n",
    "doc_topic_df.to_csv('tables/doc_topic_69_76.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construction below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt ='Luis lives in Zurich.'\n",
    "\n",
    "out_ = triplet_extractor(txt, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = triplet_extractor.tokenizer.batch_decode(out_['output_ids'][0])\n",
    "\n",
    "extracted_triplets = extract_triplets(extracted_text[0])\n",
    "print(extracted_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_persons(txt):\n",
    "    document = nlp(txt)\n",
    "\n",
    "    edited_txt = \"\"\n",
    "    for ent in document:\n",
    "        \n",
    "        if ent.ent_type_=='PERSON':\n",
    "            if ent.whitespace_:\n",
    "                edited_txt += 'Person'+ ' '\n",
    "            else:\n",
    "                edited_txt += 'Person'\n",
    "        else:\n",
    "            if ent.whitespace_:\n",
    "                edited_txt += ent.text+ ' '\n",
    "            else:\n",
    "                edited_txt += ent.text\n",
    "    \n",
    "    return edited_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "doc_df = pd.read_csv('tables/doc_69_76v30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = doc_df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_txt_list = list(map(lambda x: remove_persons(x), txt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_txt_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# Extract keywords\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(txt_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"plots/free_text_list\", \"rb\") as fp:\n",
    "        free_text_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Relations between Greece and Turkey began in the 1830s following Greece's formation after its declaration of independence from the Ottoman Empire. Modern relations began when Turkey declared its formation in 1923 following the defeat of the Ottoman Empire in World War I. \\\n",
    "Greece and Turkey have a rivalry with a history of events that have been used to justify their nationalism.[1][2] These events include the population exchange between Greece and Turkey, the Istanbul pogrom and Cypriot intercommunal violence. Greek-Turkish feuding was not a significant factor in international relations from 1930 to 1955, and during the Cold War, domestic and bipolar politics limited competitive behaviour against each other.[3][4] By the mid-1990s and later decades, the restraint on their rivalry was removed, and both nations had become each other's biggest security risk.[5][6]\\\n",
    "Control of the eastern Mediterranean and Aegean seas remain the basis of the countries' rivalry. Following the end of World War II, the UNCLOS treaty, the decolonisation of Cyprus, and the addition of the Dodecanese to Greece's territory have caused turbulence in the relationship. Several issues frequently affect their current relations, including territorial disputes over the sea and air, minority rights, and Turkey's relationship with the European Union (EU) and its member statesâ€”especially Cyprus.[7][8] Control of energy pipelines is also an increasing focus in their relations.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword-ne KG below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokberk/miniconda3/envs/ml4hc_project2/lib/python3.9/site-packages/spacy/util.py:877: UserWarning:\n",
      "\n",
      "[W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yake\n",
    "import sklearn.feature_extraction.text as text\n",
    "import spacy\n",
    "import pickle \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphdatascience import GraphDataScience\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, MDS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ner(sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    named_entities = []\n",
    "\n",
    "    entity_chunk = None #(entity,type)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.ent_iob_ == 'O':\n",
    "            if entity_chunk:\n",
    "                named_entities.append(entity_chunk)\n",
    "                entity_chunk = None\n",
    "        elif token.ent_iob_ == 'B':\n",
    "            if entity_chunk:\n",
    "                named_entities.append(entity_chunk)\n",
    "                entity_chunk = None\n",
    "            entity_chunk = (token.text,token.ent_type_)\n",
    "        else:\n",
    "            entity_chunk_text = entity_chunk[0]\n",
    "            entity_chunk_type = entity_chunk[1]\n",
    "            entity_chunk = (entity_chunk_text+' '+token.text,entity_chunk_type)\n",
    "\n",
    "\n",
    "    uninformative_entities = ['DATE','TIME','QUANTITY','ORDINAL','CARDINAL','QUANTITY','MONEY','PERCENT','PERSON']\n",
    "\n",
    "    named_entities = list(filter(lambda x: True if x[1] not in uninformative_entities else False, named_entities))\n",
    "    named_entities = np.unique(named_entities,axis=0) \n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.read_csv('tables/doc_69_76.csv')\n",
    "id_to_text_list = doc_df['id_to_text'].values\n",
    "free_text_list = doc_df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan\n",
    "# named entities\n",
    "# lemmatize -> only include named entities occuring more than k\n",
    "\n",
    "# keywords\n",
    "# using named entities as stop words for keyword extraction? -> try this\n",
    "# lemmatization before keyword extraction!\n",
    "# similarly: only include keywords occuring more than x\n",
    "# try for sure keybert\n",
    "\n",
    "# extension\n",
    "# doing the entire process with only ne, and then maybe with only keywords with specific pos tags???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner dict loaded.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('ne2vec/ner_dict'):\n",
    "    with open(\"ne2vec/ner_dict\", \"rb\") as fp:\n",
    "        ner_dict = pickle.load(fp)\n",
    "    print('ner dict loaded.')\n",
    "\n",
    "else:\n",
    "    ner_dict = {}\n",
    "\n",
    "    for idx,text in enumerate(free_text_list):\n",
    "\n",
    "        if not(isinstance(text, float) and math.isnan(text)): # check if NaN\n",
    "            id_to_text = id_to_text_list[idx]\n",
    "            ne_list = apply_ner(text)\n",
    "\n",
    "            for ne_tuple in ne_list:\n",
    "                ne = ne_tuple[0]\n",
    "\n",
    "                if not ner_dict.get(ne,None):\n",
    "                    ner_dict[ne] = [id_to_text]\n",
    "                else:\n",
    "                    ner_dict[ne].append(id_to_text)\n",
    "    \n",
    "    with open('ne2vec/ner_dict', 'wb') as f:\n",
    "        pickle.dump(ner_dict, f)\n",
    "    print('ner dict computed and saved.')\n",
    "\n",
    "\n",
    "min_ne_count = 50\n",
    "copy_ner_dict = copy.deepcopy(ner_dict)\n",
    "\n",
    "for key in copy_ner_dict:\n",
    "    \n",
    "    if len(ner_dict[key]) < min_ne_count:\n",
    "        del ner_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gokberk/Desktop/eth_courses/decipher-frus/analysis.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/decipher-frus/analysis.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     id_list \u001b[39m=\u001b[39m ner_dict[key]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/decipher-frus/analysis.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m id_list:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/decipher-frus/analysis.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         ne2doc_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat((ne2doc_df, \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/decipher-frus/analysis.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                 pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mid_to_text\u001b[39;49m\u001b[39m'\u001b[39;49m:[\u001b[39mid\u001b[39;49m],\u001b[39m'\u001b[39;49m\u001b[39mnamed_entity\u001b[39;49m\u001b[39m'\u001b[39;49m:[key]})),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/decipher-frus/analysis.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                 ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gokberk/Desktop/eth_courses/decipher-frus/analysis.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ne2doc_df\u001b[39m.\u001b[39mto_parquet(\u001b[39m'\u001b[39m\u001b[39mne2vec/ne2doc_df.parquet\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml4hc_project2/lib/python3.9/site-packages/pandas/core/frame.py:621\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dtype(dtype)\n\u001b[0;32m--> 621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(data, DataFrame):\n\u001b[1;32m    622\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39m_mgr\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (BlockManager, ArrayManager)):\n\u001b[1;32m    625\u001b[0m     \u001b[39m# first check if a Manager is passed without any other arguments\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     \u001b[39m# -> use fastpath (without checking Manager type)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ne2doc_df = pd.DataFrame(columns=['id_to_text','named_entity'])\n",
    "\n",
    "\n",
    "for key in ner_dict:\n",
    "\n",
    "    id_list = ner_dict[key]\n",
    "\n",
    "    for id in id_list:\n",
    "\n",
    "        ne2doc_df = pd.concat((ne2doc_df, \n",
    "                                pd.DataFrame({'id_to_text':[id],'named_entity':[key]})),\n",
    "                                ignore_index=True)\n",
    "\n",
    "ne2doc_df.to_parquet('ne2vec/ne2doc_df.parquet')\n",
    "# DONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT\n",
    "#lemmatization procedure (maybe used)\n",
    "\n",
    "lemmatized_ner_dict = {}\n",
    "\n",
    "for key in ner_dict:\n",
    "\n",
    "    lemmatized_ne = \" \".join([token.lemma_ for token in nlp(key.lower())])\n",
    "    lemmatized_ner_dict[lemmatized_ne] = lemmatized_ner_dict.get(lemmatized_ne,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19512/19512 [1:04:18<00:00,  5.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# exclude found named entities to avoid duplication\n",
    "stop_words=text.ENGLISH_STOP_WORDS.union(list(map(lambda x: x.lower(),list(ner_dict.keys()))))\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan='en', n=1, top=20,dedupLim=0.9, stopwords=stop_words)\n",
    "\n",
    "custom_voc = set()\n",
    "keyword_docs = []\n",
    "\n",
    "for free_text in tqdm(free_text_list):\n",
    "    if not(isinstance(free_text, float) and math.isnan(free_text)): # check if NaN\n",
    "        keywords = custom_kw_extractor.extract_keywords(nlp(free_text.lower()).text)\n",
    "        keywords = set(map(lambda x: x[0],keywords))\n",
    "\n",
    "        keyword_docs.append(\" \".join(keywords))\n",
    "        custom_voc.update(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ne2vec/keyword_docs', 'wb') as f:\n",
    "        pickle.dump(keyword_docs, f)\n",
    "with open('ne2vec/custom_voc', 'wb') as f:\n",
    "        pickle.dump(custom_voc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.2,min_df=30)\n",
    "\n",
    "X = vectorizer.fit_transform(keyword_docs)\n",
    "\n",
    "processed_key2doc_mat = vectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19511/19511 [22:49<00:00, 14.25it/s]\n"
     ]
    }
   ],
   "source": [
    "ke2doc_df = pd.DataFrame(columns=['id_to_text','named_entity']) # 'named_entity' is actually keyword/\n",
    "\n",
    "\n",
    "for idx,temp_keyword_list in enumerate(tqdm(processed_key2doc_mat)):\n",
    "\n",
    "    for temp_keyword in temp_keyword_list:\n",
    "\n",
    "        ke2doc_df = pd.concat((ke2doc_df, \n",
    "                                pd.DataFrame({'id_to_text':[idx],'named_entity':[temp_keyword]})),\n",
    "                                ignore_index=True)\n",
    "\n",
    "ke2doc_df['id_to_text'] = ke2doc_df['id_to_text'].astype(str)\n",
    "ke2doc_df.to_parquet('ne2vec/ke2doc_df.parquet')\n",
    "# DONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = GraphDataScience(\"bolt://localhost:7687\", auth=('neo4j', 'bos'), database='keyword2vec')\n",
    "\n",
    "embedding_df = gds.run_cypher(\n",
    "    \"\"\"\n",
    "        match (e:Entity)\n",
    "        return e.name as entity, e['fastrp-embedding'] as fastrp_embedding, e['node2vec-embedding'] as node2vec_embedding\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokberk/miniconda3/envs/ml4hc_project2/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning:\n",
      "\n",
      "The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "\n",
      "/Users/gokberk/miniconda3/envs/ml4hc_project2/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning:\n",
      "\n",
      "The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "reduced_emb_mat = TSNE(n_components=2, perplexity=50).fit_transform(np.stack(embedding_df['node2vec_embedding']))\n",
    "x,y = reduced_emb_mat[:,0],reduced_emb_mat[:,1]\n",
    "\n",
    "'''matplotlib\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.scatter(x,y)\n",
    "for i, txt in enumerate(embedding_df['entity'].values):\n",
    "    ax.annotate(txt, (x[i], y[i]))'''\n",
    "\n",
    "# plotly\n",
    "fig = px.scatter(x=x, y=y, text=embedding_df['entity'].values, width=900, height=900)\n",
    "fig.write_html(\"ne2vec/only_ke_emb_node2vec.html\")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_mat = cosine_similarity(np.stack(embedding_df['fastrp_embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, top_n):\n",
    "\n",
    "    word_idx = embedding_df[embedding_df['entity']==word].index[0]\n",
    "\n",
    "    similar_entity_idx = np.argsort(cossim_mat[word_idx])[::-1][1:top_n+1]\n",
    "\n",
    "    similar_entity_names = embedding_df['entity'].values[similar_entity_idx]\n",
    "    similar_entity_sims = cossim_mat[word_idx][similar_entity_idx]\n",
    "\n",
    "    return np.array([similar_entity_names,similar_entity_sims]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['given', 0.9997970545229851],\n",
       "       ['private', 0.9997787087634654],\n",
       "       ['end', 0.9997768219522268],\n",
       "       ['progress', 0.9997757420410192],\n",
       "       ['strong', 0.999774906026226],\n",
       "       ['terms', 0.9997710754629248],\n",
       "       ['case', 0.9997648636837971],\n",
       "       ['expressed', 0.9997574591859688],\n",
       "       ['fact', 0.9997565880692844],\n",
       "       ['leaders', 0.9997526278924671]], dtype=object)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('change',10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
