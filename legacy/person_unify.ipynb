{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import glob \n",
    "import math\n",
    "import itertools\n",
    "import jellyfish\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import copy\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ray\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# define namespaces in FRUS schema\n",
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus': 'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi': 'http://www.w3.org/2001/XInclude'\n",
    "      }\n",
    "\n",
    "# define path to save extracted files\n",
    "tables_path = 'tables/tables_52_88_demo/'\n",
    "\n",
    "# only use documents within these years\n",
    "start_year, end_year = 1952, 1988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person(item, file):\n",
    "    volume = file[8:-4]\n",
    "\n",
    "    persName_item = item.find('.//dflt:persName[@xml:id]', ns)\n",
    "\n",
    "    if persName_item is not None:\n",
    "\n",
    "        persName_text = \"\".join(persName_item.itertext())\n",
    "        person_id = persName_item.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "        all_text = \"\".join(item.itertext())\n",
    "        end_idx = all_text.find(persName_text) + len(persName_text+',')\n",
    "        person_descp = \" \".join(all_text[end_idx:].split())\n",
    "\n",
    "        person_name = \" \".join(re.sub(',','',\" \".join(persName_text.split(', ')[::-1])).split())\n",
    "\n",
    "        person_id = volume + '_' + person_id\n",
    "\n",
    "        #global person_df\n",
    "        #person_df = pd.concat((person_df, pd.DataFrame({'id':[person_id],\n",
    "        #                                            'name':[person_name],\n",
    "        #                                            'description':[person_descp]})),ignore_index=True)\n",
    "    return {'id':person_id,'name':person_name,'description':person_descp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/543 [00:00<00:16, 31.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person annotation in volumes/frus1952-54v01p2.xml.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 49/543 [00:03<00:21, 22.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person annotation in volumes/frus1952-54v01p1.xml.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 190/543 [00:12<00:12, 27.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person annotation in volumes/frus1977-80v09.xml.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 428/543 [00:26<00:08, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person annotation in volumes/frus1952-54v12p1.xml.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 455/543 [00:29<00:07, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person annotation in volumes/frus1952-54v12p2.xml.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [00:37<00:00, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not annotated volume count: 5\n",
      "Row count: 48363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#person_df = pd.DataFrame(columns=['id','name','description'])\n",
    "global_person_list = []\n",
    "\n",
    "no_annotation_cnt = 0\n",
    "\n",
    "for file in tqdm(glob.glob('volumes/frus*')):\n",
    "    file_start_year = int(file[12:16])\n",
    "    \n",
    "    if file_start_year >= start_year and file_start_year<=end_year:\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        persons_section = root.find(\"./dflt:text/dflt:front//dflt:div[@xml:id='persons']\", ns)\n",
    "        \n",
    "        if persons_section:\n",
    "            for item in persons_section.findall('.//dflt:item/dflt:hi/dflt:persName[@xml:id]/../..', ns):\n",
    "                person_dict = extract_person(item,file)\n",
    "                global_person_list.append(person_dict) \n",
    "            for item in persons_section.findall('.//dflt:item/dflt:persName[@xml:id]/..', ns):\n",
    "                person_dict = extract_person(item,file)\n",
    "                global_person_list.append(person_dict) \n",
    "        else:\n",
    "            print(f'No person annotation in {file}.')\n",
    "            no_annotation_cnt += 1\n",
    "\n",
    "person_df = pd.DataFrame(global_person_list)\n",
    "print(f'Not annotated volume count: {no_annotation_cnt}')\n",
    "print(f'Row count: {len(person_df)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: reduce exactly matched names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 19352\n",
      "Step 1 finished.\n"
     ]
    }
   ],
   "source": [
    "unified_person_dict = {}\n",
    "\n",
    "def aux(row):\n",
    "    global unified_person_dict\n",
    "\n",
    "    if row['name'] in unified_person_dict:\n",
    "      \n",
    "      temp_dict = unified_person_dict[row['name']]\n",
    "\n",
    "      temp_dict['id_list'].append(row['id'])\n",
    "      temp_dict['description_list'].append(row['description'])\n",
    "    \n",
    "    else:\n",
    "      unified_person_dict[row['name']]= {'id_list':[row['id']],\n",
    "                                        'description_list':[row['description']]}\n",
    "\n",
    "    return\n",
    "\n",
    "person_df.apply(lambda x:aux(x), axis=1)\n",
    "unified_person_df = pd.DataFrame.from_dict(unified_person_dict,orient='index').reset_index(drop=False)\n",
    "unified_person_df.rename(columns={'index':'name'}, inplace=True)\n",
    "print(f'Row count: {len(unified_person_df)}')\n",
    "print('Step 1 finished.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: reduce names with exactly same words but different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 17633\n",
      "Step 2 finished.\n"
     ]
    }
   ],
   "source": [
    "unified_person_df['name_set'] = unified_person_df.name.apply(lambda x: \" \".join(sorted(x.split())))\n",
    "\n",
    "new_unified_person_dict = {}\n",
    "\n",
    "def aux2(row):\n",
    "    global new_unified_person_dict\n",
    "\n",
    "    if row['name_set'] in new_unified_person_dict:\n",
    "      \n",
    "        temp_dict = new_unified_person_dict[row['name_set']]\n",
    "\n",
    "        temp_dict['name_list'].append(row['name'])\n",
    "        temp_dict['id_list'] += row['id_list']\n",
    "        temp_dict['description_list'] += row['description_list']\n",
    "    \n",
    "    else:\n",
    "        new_unified_person_dict[row['name_set']]= {'name_list':[row['name']],\n",
    "                                                    'id_list':row['id_list'],\n",
    "                                                    'description_list':row['description_list']}\n",
    "\n",
    "    return\n",
    "\n",
    "unified_person_df.apply(lambda x:aux2(x), axis=1)\n",
    "\n",
    "new_unified_person_df = pd.DataFrame.from_dict(new_unified_person_dict,orient='index').reset_index(drop=False)\n",
    "new_unified_person_df.rename(columns={'index':'name_set'}, inplace=True)\n",
    "print(f'Row count: {len(new_unified_person_df)}')\n",
    "print('Step 2 finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_set</th>\n",
       "      <th>name_list</th>\n",
       "      <th>id_list</th>\n",
       "      <th>description_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acheson Dean</td>\n",
       "      <td>[Dean Acheson, Acheson Dean]</td>\n",
       "      <td>[frus1964-68v03_p_AD1, frus1969-76v38p1_p_AD_1...</td>\n",
       "      <td>[Secretary of State from 1949 until 1953, Secr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Boggs Hale</td>\n",
       "      <td>[Hale Boggs, Boggs Hale]</td>\n",
       "      <td>[frus1964-68v03_p_BH1, frus1964-68v02_p_BH1, f...</td>\n",
       "      <td>[Democratic Representative from Louisiana, Dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Brown Harold</td>\n",
       "      <td>[Harold Brown, Brown Harold]</td>\n",
       "      <td>[frus1964-68v03_p_BH2, frus1964-68v02_p_BH2, f...</td>\n",
       "      <td>[Director, Defense Research and Engineering, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bui Diem</td>\n",
       "      <td>[Bui Diem, Diem Bui]</td>\n",
       "      <td>[frus1964-68v03_p_BD1, frus1969-76v14_p_BD5, f...</td>\n",
       "      <td>[Vietnamese Chief of Staff in the Quat governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bundy McGeorge</td>\n",
       "      <td>[Bundy McGeorge, McGeorge Bundy]</td>\n",
       "      <td>[frus1964-68v03_p_BMG1, frus1961-63v11_p_BMG2,...</td>\n",
       "      <td>[President’s Special Assistant for National Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16876</th>\n",
       "      <td>Aidit Dipa Nusantara</td>\n",
       "      <td>[Aidit Dipa Nusantara, Dipa Nusantara Aidit]</td>\n",
       "      <td>[frus1961-63v23_p_ADN1, frus1964-68v26_p_ADN1]</td>\n",
       "      <td>[leader of the PKI (Partai Komunis Indonesia/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>Cater Douglass Jr. S.</td>\n",
       "      <td>[S. Douglass Jr. Cater, Jr. S. Douglass Cater]</td>\n",
       "      <td>[frus1964-68v31_p_CSDJ1, frus1964-68v32_p_CSDJ1]</td>\n",
       "      <td>[Special Assistant to the President July 1965–...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17040</th>\n",
       "      <td>J. Jr. Miguel Moreno</td>\n",
       "      <td>[Jr. Miguel J. Moreno, Miguel J. Jr. Moreno]</td>\n",
       "      <td>[frus1964-68v31_p_MMJJ1, frus1958-60v05_p_MMJJ1]</td>\n",
       "      <td>[Panamanian Representative to the Council of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17198</th>\n",
       "      <td>Henderson Loy</td>\n",
       "      <td>[Henderson Loy, Loy Henderson]</td>\n",
       "      <td>[frus1958-60v11_p_HLW1, frus1958-60v05_p_HL1]</td>\n",
       "      <td>[Deputy Under Secretary of State for Administr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17520</th>\n",
       "      <td>Amory Derick Heathcoat</td>\n",
       "      <td>[Derick Heathcoat Amory, Heathcoat Amory Derick]</td>\n",
       "      <td>[frus1958-60v04_p_ADH1, frus1958-60v04_p_HAD1]</td>\n",
       "      <td>[see Heathcoat Amory, Derick, British Chancell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1637 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name_set  \\\n",
       "0                Acheson Dean   \n",
       "9                  Boggs Hale   \n",
       "14               Brown Harold   \n",
       "17                   Bui Diem   \n",
       "20             Bundy McGeorge   \n",
       "...                       ...   \n",
       "16876    Aidit Dipa Nusantara   \n",
       "16996   Cater Douglass Jr. S.   \n",
       "17040    J. Jr. Miguel Moreno   \n",
       "17198           Henderson Loy   \n",
       "17520  Amory Derick Heathcoat   \n",
       "\n",
       "                                              name_list  \\\n",
       "0                          [Dean Acheson, Acheson Dean]   \n",
       "9                              [Hale Boggs, Boggs Hale]   \n",
       "14                         [Harold Brown, Brown Harold]   \n",
       "17                                 [Bui Diem, Diem Bui]   \n",
       "20                     [Bundy McGeorge, McGeorge Bundy]   \n",
       "...                                                 ...   \n",
       "16876      [Aidit Dipa Nusantara, Dipa Nusantara Aidit]   \n",
       "16996    [S. Douglass Jr. Cater, Jr. S. Douglass Cater]   \n",
       "17040      [Jr. Miguel J. Moreno, Miguel J. Jr. Moreno]   \n",
       "17198                    [Henderson Loy, Loy Henderson]   \n",
       "17520  [Derick Heathcoat Amory, Heathcoat Amory Derick]   \n",
       "\n",
       "                                                 id_list  \\\n",
       "0      [frus1964-68v03_p_AD1, frus1969-76v38p1_p_AD_1...   \n",
       "9      [frus1964-68v03_p_BH1, frus1964-68v02_p_BH1, f...   \n",
       "14     [frus1964-68v03_p_BH2, frus1964-68v02_p_BH2, f...   \n",
       "17     [frus1964-68v03_p_BD1, frus1969-76v14_p_BD5, f...   \n",
       "20     [frus1964-68v03_p_BMG1, frus1961-63v11_p_BMG2,...   \n",
       "...                                                  ...   \n",
       "16876     [frus1961-63v23_p_ADN1, frus1964-68v26_p_ADN1]   \n",
       "16996   [frus1964-68v31_p_CSDJ1, frus1964-68v32_p_CSDJ1]   \n",
       "17040   [frus1964-68v31_p_MMJJ1, frus1958-60v05_p_MMJJ1]   \n",
       "17198      [frus1958-60v11_p_HLW1, frus1958-60v05_p_HL1]   \n",
       "17520     [frus1958-60v04_p_ADH1, frus1958-60v04_p_HAD1]   \n",
       "\n",
       "                                        description_list  \n",
       "0      [Secretary of State from 1949 until 1953, Secr...  \n",
       "9      [Democratic Representative from Louisiana, Dem...  \n",
       "14     [Director, Defense Research and Engineering, D...  \n",
       "17     [Vietnamese Chief of Staff in the Quat governm...  \n",
       "20     [President’s Special Assistant for National Se...  \n",
       "...                                                  ...  \n",
       "16876  [leader of the PKI (Partai Komunis Indonesia/I...  \n",
       "16996  [Special Assistant to the President July 1965–...  \n",
       "17040  [Panamanian Representative to the Council of t...  \n",
       "17198  [Deputy Under Secretary of State for Administr...  \n",
       "17520  [see Heathcoat Amory, Derick, British Chancell...  \n",
       "\n",
       "[1637 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for observation\n",
    "new_unified_person_df[new_unified_person_df['name_list'].apply(lambda x: len(x)==2)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: find and reduce near-duplicate names + obvious misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = new_unified_person_df['name_set'].values\n",
    "\n",
    "def compute_sim(s1,func,s2):\n",
    "    return func(s1,s2)\n",
    "\n",
    "def compute_exact_word_overlap(s1,s2):\n",
    "    l1 = set([x for x in list(set(tokenizer.tokenize(s1))) if len(x)>=3])\n",
    "    l2 = set([x for x in list(set(tokenizer.tokenize(s2))) if len(x)>=3])\n",
    "\n",
    "    return len(l1.intersection(l2))\n",
    "\n",
    "@ray.remote\n",
    "def find_matches(idx):\n",
    "    s2 = all_names[idx]\n",
    "    \n",
    "    spiro_dist_df = pd.DataFrame({'name_set':all_names,\n",
    "                                'overlap_cnt':[compute_exact_word_overlap(x,s2) for x in all_names],\n",
    "                                'dam_lev_dist':[compute_sim(x, jellyfish.damerau_levenshtein_distance,s2) for x in all_names],\n",
    "                                'jaro_sim':[compute_sim(x, jellyfish.jaro_winkler_similarity,s2) for x in all_names]})\n",
    "    \n",
    "    # misspelling check - hyperparameter\n",
    "    misspelling_idx = set(spiro_dist_df[(spiro_dist_df['dam_lev_dist'] <=1)].index.values)\n",
    "\n",
    "    # near-duplication check - hyperparameter\n",
    "    spiro_dist_df = spiro_dist_df[spiro_dist_df['overlap_cnt']>=2]\n",
    "    match_idx = set(spiro_dist_df[(spiro_dist_df['jaro_sim'] >= 0.9) | (spiro_dist_df['dam_lev_dist'] <=5)].index.values)\n",
    "\n",
    "    return match_idx.union(misspelling_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name : matched names dict\n",
    "ray.init(num_cpus=13)\n",
    "\n",
    "futures = [find_matches.remote(idx) for idx in range(len(all_names))]\n",
    "result_tuple_list = ray.get(futures)\n",
    "ray.shutdown()\n",
    "\n",
    "t = {}\n",
    "for idx in range(len(all_names)):\n",
    "    t[idx]=result_tuple_list[idx]\n",
    "\n",
    "\n",
    "# code to merge matches\n",
    "# finds friend of friend is friend!\n",
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')\n",
    "    \n",
    "# reduce matched names into single entry\n",
    "for temp_key in t:\n",
    "    \n",
    "    te_df = new_unified_person_df.iloc[list(t[temp_key])]\n",
    "\n",
    "    name_list = list(itertools.chain.from_iterable(te_df['name_list'].values))\n",
    "    id_list = list(itertools.chain.from_iterable(te_df['id_list'].values))\n",
    "    description_list = list(itertools.chain.from_iterable(te_df['description_list'].values))\n",
    "\n",
    "    new_unified_person_df.at[temp_key, 'name_list'] = name_list\n",
    "    new_unified_person_df.at[temp_key, 'id_list'] = id_list\n",
    "    new_unified_person_df.at[temp_key, 'description_list'] = description_list\n",
    "\n",
    "new_unified_person_df = new_unified_person_df.loc[t.keys()]\n",
    "\n",
    "# save unified person table\n",
    "new_unified_person_df.to_parquet(tables_path+'unified_person_df_step3.parquet')\n",
    "print(f'Row count: {len(new_unified_person_df)}')\n",
    "print('Step 3 finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['McKisson Robert', 'Robert M. McKisson']),\n",
       "       list(['Luigi Einaudi', 'Einaudi Luigi', 'Luigi R. Einaudi', 'Luigi R. Einaui']),\n",
       "       list(['Harry Bergold', 'Jr. Harry E Bergold']),\n",
       "       list(['Nuseibeh Anwar', 'Anwar Nuseibeh']),\n",
       "       list(['Armand Berard', 'Berard Armand', 'Bérard Armand', 'Armand Bérard']),\n",
       "       list(['Dennis B. Ross', 'Dennis Ross']),\n",
       "       list(['Ali Mohammad', 'Mohammad Ali', 'Ali Mohammad Khan', 'Mohammed Ali', 'Ali Nasser Muhammad', 'Mohammad Ali Wardhana', 'Muhammad Ali', 'Mohammed Ali Rajai', 'Sher Ali Mohammad']),\n",
       "       list(['Ilter Turkmen', 'Ilter Türkmen']),\n",
       "       list(['Leslie H. Brown', 'Brown Leslie']),\n",
       "       list(['Franco Mario Malfatti', 'Franco M. Malfatti'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for observation\n",
    "new_unified_person_df[new_unified_person_df['name_list'].apply(lambda x: len(x)>=2)]['name_list'].sample(10).values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: find each person's wikidata entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "user_agent = 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'\n",
    "\n",
    "sparqlwd = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "sparqlwd.setReturnFormat(JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wiki_entity(name):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?item WHERE {\n",
    "        SERVICE wikibase:mwapi {\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search  \\'\"\"\"+name+\"\"\"\\';\n",
    "                            mwapi:language \"en\".\n",
    "            ?item wikibase:apiOutputItem mwapi:item.\n",
    "            ?num wikibase:apiOrdinal true.\n",
    "        }\n",
    "        ?item wdt:P31 wd:Q5\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {name}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "@ray.remote\n",
    "def process_name_list(name_list):\n",
    "    \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    wiki_tag = set()\n",
    "\n",
    "    for name in name_list:\n",
    "        res = find_wiki_entity(name)\n",
    "\n",
    "        for binding in res['results']['bindings']:\n",
    "            wiki_tag.add(binding['item']['value'])\n",
    "\n",
    "    return list(wiki_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(process_name_list pid=4380)\u001b[0m name: Lucius D. Clay\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4380)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4384)\u001b[0m name: Washington LaRae\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4384)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4376)\u001b[0m name: Sirimavo Bandaranaike\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4376)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4374)\u001b[0m name: Hernandez Acosta Valentin\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4374)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4381)\u001b[0m name: Beryl W. Sprinkel\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4381)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4380)\u001b[0m name: Basilio Lami Dozo\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4380)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4382)\u001b[0m name: Borel Dominique\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4382)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4380)\u001b[0m name: Gilbert E. Yates\n",
      "\u001b[2m\u001b[36m(process_name_list pid=4380)\u001b[0m error message: HTTP Error 429: Too Many Requests\n",
      "Step 4 finished.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=13)\n",
    "new_unified_person_df = pd.read_parquet(tables_path+'unified_person_df_step3.parquet')\n",
    "futures = [process_name_list.remote(row) for row in new_unified_person_df['name_list'].values]\n",
    "wiki_col = ray.get(futures)\n",
    "ray.shutdown()\n",
    "\n",
    "new_unified_person_df['wiki_col'] = wiki_col\n",
    "new_unified_person_df.to_parquet(tables_path+'unified_person_df_step4.parquet')\n",
    "print(f'Row count: {len(new_unified_person_df)}')\n",
    "print('Step 4 finished.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 5: reduce multiple candidate wikidata entities to single using sbert for each person, if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for using sbert for deciding among wikidata entries\n",
    "def get_entity_descp(Q):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?descp\n",
    "        WHERE \n",
    "        {\n",
    "        wd:\"\"\"+Q+\"\"\" schema:description ?descp.\n",
    "        FILTER ( lang(?descp) = \"en\" )\n",
    "        }\"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {Q}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_candidate_entities(row):\n",
    "\n",
    "    q_list = row['wiki_col']\n",
    "    \n",
    "    wiki_descp = []\n",
    "\n",
    "    for q in q_list:\n",
    "        \n",
    "        res = get_entity_descp(q.split('/')[-1])\n",
    "        \n",
    "        if len(res['results']['bindings'])==0:\n",
    "            wiki_descp.append('')\n",
    "        else:      \n",
    "            for binding in res['results']['bindings']:\n",
    "\n",
    "                wiki_descp.append(binding['descp']['value'])\n",
    "\n",
    "    return wiki_descp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def process_wiki_col(row):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    wiki_col = row['wiki_col']\n",
    "    \n",
    "    if len(wiki_col)==0:\n",
    "        return None\n",
    "\n",
    "    elif len(wiki_col)==1:\n",
    "        return wiki_col[0]\n",
    "\n",
    "    else:\n",
    "        desc_list = row['description_list']\n",
    "        frus_embedding = np.mean(model.encode(desc_list), axis=0)\n",
    "\n",
    "        wiki_descs = process_candidate_entities(row)\n",
    "        wiki_embeddings = model.encode(wiki_descs)\n",
    "\n",
    "        cos_sim = util.cos_sim(frus_embedding, wiki_embeddings)\n",
    "\n",
    "        selected_idx = np.argmax(cos_sim,axis=1)[0]\n",
    "        \n",
    "        return row[\"wiki_col\"][selected_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:47:20,954\tWARNING worker.py:1257 -- Warning: The remote function __main__.process_wiki_col is very large (87 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(process_wiki_col pid=4599)\u001b[0m name: Q1465644\n",
      "\u001b[2m\u001b[36m(process_wiki_col pid=4599)\u001b[0m error message: HTTP Error 429: Too Many Requests\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=8)\n",
    "new_unified_person_df_wikicol = pd.read_parquet(tables_path+'unified_person_df_step4.parquet')\n",
    "\n",
    "futures = [process_wiki_col.remote(row) for _,row in new_unified_person_df_wikicol.iterrows()]\n",
    "selected_wiki_entity = ray.get(futures)\n",
    "ray.shutdown()\n",
    "\n",
    "new_unified_person_df['selected_wiki_entity'] = selected_wiki_entity\n",
    "new_unified_person_df.to_parquet(tables_path+'unified_person_df_step5.parquet')\n",
    "print(f'Row count: {len(new_unified_person_df)}')\n",
    "print('Step 5 finished.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 6: reduce names with exactly same wikidata entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 241 keys.\n",
      "---\n",
      "removing 0 keys.\n",
      "---\n",
      "Row count: 13076\n",
      "Step 6 finished.\n"
     ]
    }
   ],
   "source": [
    "new_unified_person_df = pd.read_parquet(tables_path+'unified_person_df_step5.parquet')\n",
    "\n",
    "t = {}\n",
    "\n",
    "for idx, key in new_unified_person_df.iterrows():\n",
    "\n",
    "    ent = key['selected_wiki_entity']\n",
    "\n",
    "    if not ent:\n",
    "        t[idx]=set([idx])\n",
    "    else:\n",
    "        t[idx]=set(new_unified_person_df[new_unified_person_df['selected_wiki_entity']==ent].index)\n",
    "\n",
    "\n",
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')\n",
    "\n",
    "for temp_key in t:\n",
    "    \n",
    "    te_df = new_unified_person_df.loc[list(t[temp_key])]\n",
    "\n",
    "    name_list = list(itertools.chain.from_iterable(te_df['name_list'].values))\n",
    "    id_list = list(itertools.chain.from_iterable(te_df['id_list'].values))\n",
    "    description_list = list(itertools.chain.from_iterable(te_df['description_list'].values))\n",
    "\n",
    "    new_unified_person_df.at[temp_key, 'name_list'] = name_list\n",
    "    new_unified_person_df.at[temp_key, 'id_list'] = id_list\n",
    "    new_unified_person_df.at[temp_key, 'description_list'] = description_list\n",
    "\n",
    "new_unified_person_df = new_unified_person_df.loc[t.keys()]\n",
    "\n",
    "new_unified_person_df.to_parquet(tables_path+'unified_person_df_final.parquet')\n",
    "print(f'Row count: {len(new_unified_person_df)}')\n",
    "print('Step 6 finished.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
