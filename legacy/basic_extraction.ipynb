{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import ray\n",
    "ray.init(num_cpus=13)\n",
    "\n",
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus':'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi':'http://www.w3.org/2001/XInclude'\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from lexicalrichness import LexicalRichness\n",
    "from textblob import TextBlob\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#nlp.add_pipe('spacytextblob')\n",
    "\n",
    "tables_path = 'tables/tables_52_62/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [00:08<00:00, 61.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# ENTITY(\"DOCUMENT\")\n",
    "# ENTITY(\"PERSON_SENTBY\")\n",
    "# ENTITY(\"PERSON_SENTTO\")\n",
    "# ENTITY(\"PERSON_MENTIONED\")\n",
    "# ENTITY(\"INST_MENTIONED\")\n",
    "\n",
    "@ray.remote\n",
    "def extract_document(doc, volume):\n",
    "\n",
    "    #global doc_df\n",
    "\n",
    "    #global person_sentby_df\n",
    "    #global person_sentto_df\n",
    "    #global person_mentioned_df\n",
    "\n",
    "    #global instution_mentioned_df\n",
    "\n",
    "    # id\n",
    "    id_to_text = volume + '_' + doc.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "    # subtype\n",
    "    subtype = doc.attrib['subtype']\n",
    "\n",
    "    # date and year and era\n",
    "    date = None\n",
    "    year = None\n",
    "    era = None\n",
    "    if subtype!='editorial-note':\n",
    "        fmt = doc.attrib['{http://history.state.gov/frus/ns/1.0}doc-dateTime-max']\n",
    "        date = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d')\n",
    "        year = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d').year\n",
    "        era = era_df[(era_df['startDate'] <= date) & (era_df['endDate'] > date)].president.values[0]\n",
    "\n",
    "    # source\n",
    "    source_tag = doc.find('.//dflt:note[@type=\"source\"]',ns)\n",
    "    if source_tag is not None:\n",
    "        source = \" \".join(ET.tostring(source_tag, encoding='unicode', method='text').split())\n",
    "    else:\n",
    "        source = None\n",
    "\n",
    "    # title -includes removing note tag!\n",
    "    head_tag = doc.find('./dflt:head', ns)\n",
    "    child_note_tags = head_tag.findall('./dflt:note', ns)\n",
    "\n",
    "    for note_tag in child_note_tags:\n",
    "        head_tag.remove(note_tag)\n",
    "\n",
    "    title = \" \".join(ET.tostring(head_tag, encoding='unicode', method='text').split())\n",
    "\n",
    "    # city\n",
    "    place_tag = doc.find('.//dflt:placeName',ns)\n",
    "    if place_tag is not None:\n",
    "        txt = \"\".join(place_tag.itertext())\n",
    "        txt = \" \".join(txt.split())\n",
    "        txt = \" \".join(txt.split(',')[0].split())\n",
    "        city = city_lookup_dict[txt]\n",
    "    else:\n",
    "        city = None\n",
    "\n",
    "    # person_sentby\n",
    "    person_sentby = []\n",
    "    person_sentby_dict_list = []\n",
    "\n",
    "    for pers_tag in doc.findall('.//dflt:persName[@type=\"from\"]',ns):\n",
    "        if pers_tag is not None: \n",
    "            if 'corresp' in pers_tag.attrib:\n",
    "                if pers_tag.attrib['corresp'][0]=='#':\n",
    "                    person_id = pers_tag.attrib['corresp'][1:]\n",
    "                else:\n",
    "                    person_id = pers_tag.attrib['corresp']\n",
    "                person_id = volume + '_' + person_id\n",
    "                person_name = person_id_lookup_dict.get(person_id,None)\n",
    "                if person_name:\n",
    "                    person_sentby.append(person_name)\n",
    "                    person_sentby_dict_list.append({'person_name':person_name,'sent':id_to_text})\n",
    "            else:\n",
    "                txt = (\" \".join(pers_tag.itertext()))\n",
    "                txt = \" \".join(txt.split())\n",
    "                person_sentby.append(txt)\n",
    "\n",
    "    #docs[0].findall('.//dflt:list',ns)[0].attrib #list -not included yet-\n",
    "\n",
    "    signed_person_tag = doc.find('.//dflt:signed//dflt:persName',ns)\n",
    "    if signed_person_tag is not None:\n",
    "        if 'corresp' in signed_person_tag.attrib:\n",
    "            person_id = signed_person_tag.attrib['corresp'][1:]\n",
    "            if signed_person_tag.attrib['corresp'][0]=='#':\n",
    "                person_id = signed_person_tag.attrib['corresp'][1:]\n",
    "            else:\n",
    "                person_id = signed_person_tag.attrib['corresp']\n",
    "            person_id = volume + '_' + person_id\n",
    "            person_name = person_id_lookup_dict.get(person_id,None)\n",
    "            if person_name:\n",
    "                person_sentby.append(person_name)\n",
    "                person_sentby_dict_list.append({'person_name':person_name,'sent':id_to_text})\n",
    "        else:\n",
    "            txt = (\" \".join(signed_person_tag.itertext()))\n",
    "            txt = \" \".join(txt.split())\n",
    "            person_sentby.append(txt)\n",
    "\n",
    "    # person_sentto\n",
    "    person_sentto = []\n",
    "    person_sentto_dict_list = []\n",
    "\n",
    "    for pers_tag in doc.findall('.//dflt:persName[@type=\"to\"]',ns):\n",
    "        if pers_tag is not None:\n",
    "            if 'corresp' in pers_tag.attrib:\n",
    "                if pers_tag.attrib['corresp'][0]=='#':\n",
    "                    person_id = pers_tag.attrib['corresp'][1:]\n",
    "                else:\n",
    "                    person_id = pers_tag.attrib['corresp']\n",
    "                person_id = volume + '_' + person_id\n",
    "                person_name = person_id_lookup_dict.get(person_id,None)\n",
    "                if person_name:\n",
    "                    person_sentto.append(person_name)\n",
    "                    person_sentto_dict_list.append({'person_name':person_name,'received':id_to_text})\n",
    "            else:\n",
    "                txt = (\" \".join(pers_tag.itertext()))\n",
    "                txt = \" \".join(txt.split())\n",
    "                person_sentto.append(txt)\n",
    "\n",
    "    #docs[0].findall('.//dflt:list[@type=\"to\"]',ns)[0].attrib # list -not included yet-\n",
    "\n",
    "\n",
    "    # inst_sentby\n",
    "    inst_sentby = []\n",
    "\n",
    "    for gloss_tag in doc.findall('.//dflt:gloss[@type=\"from\"]',ns):\n",
    "\n",
    "        txt = (\" \".join(gloss_tag.itertext()))\n",
    "        txt = \" \".join(txt.split())\n",
    "        inst_sentby.append(txt)\n",
    "\n",
    "    # inst_sentto\n",
    "    inst_sentto = []\n",
    "\n",
    "    for gloss_tag in doc.findall('.//dflt:gloss[@type=\"to\"]',ns):\n",
    "\n",
    "        txt = (\" \".join(gloss_tag.itertext()))\n",
    "        txt = \" \".join(txt.split())\n",
    "        inst_sentto.append(txt)\n",
    "\n",
    "\n",
    "    # person_mentioned -includes removing note tag!\n",
    "    person_mentioned = set()\n",
    "    person_mentioned_dict_list = []\n",
    "\n",
    "    notes_parent_tags = doc.findall('.//dflt:note/..',ns)\n",
    "\n",
    "    for parent_tag in notes_parent_tags:\n",
    "\n",
    "        for note_tag in parent_tag.findall('./dflt:note',ns):\n",
    "            parent_tag.remove(note_tag)\n",
    "\n",
    "\n",
    "    pers_tags = doc.findall('.//dflt:persName[@corresp]',ns)\n",
    "    for temp_tag in pers_tags:\n",
    "        if temp_tag.attrib['corresp'][0]=='#':\n",
    "            person_id = temp_tag.attrib['corresp'][1:]\n",
    "        else:\n",
    "            person_id = temp_tag.attrib['corresp']\n",
    "        person_id = volume + '_' + person_id\n",
    "        person_name = person_id_lookup_dict.get(person_id,None)\n",
    "        if person_name:\n",
    "            person_mentioned.add(person_name)\n",
    "            person_mentioned_dict_list.append({'person_name':person_name,'mentioned_in':id_to_text})\n",
    "\n",
    "\n",
    "    # inst_mentioned -includes removing note tag!\n",
    "    instution_mentioned = set()\n",
    "    institution_mentioned_dict_list = []\n",
    "\n",
    "    inst_tags = doc.findall('.//dflt:gloss[@target]',ns)\n",
    "    for temp_tag in inst_tags:\n",
    "        if temp_tag.attrib['target'][0]=='#':\n",
    "            term_id = temp_tag.attrib['target'][1:]\n",
    "        else:\n",
    "            term_id = temp_tag.attrib['target']\n",
    "        term_id = volume + '_' + term_id\n",
    "        term_description_set = institution_id_lookup_dict.get(term_id,None)\n",
    "        if term_description_set:\n",
    "            instution_mentioned.add(term_description_set)\n",
    "            institution_mentioned_dict_list.append({'description_set':term_description_set,'mentioned_in':id_to_text})\n",
    "\n",
    "\n",
    "    # free text\n",
    "    free_text = \"\"\n",
    "\n",
    "    tag_list = doc.findall('./*',ns)\n",
    "    \n",
    "    # find free text's start and end elements\n",
    "    lidx,ridx = 0,0\n",
    "\n",
    "    for idx,tag in enumerate(tag_list):\n",
    "        if tag.tag not in not_text_tags:\n",
    "            lidx = idx\n",
    "            break\n",
    "    \n",
    "    for idx,tag in enumerate(tag_list[::-1]):\n",
    "        if tag.tag in text_tags:\n",
    "            ridx = len(tag_list)-1-idx\n",
    "            break\n",
    "    \n",
    "    # remove all <note> in free text\n",
    "    notes_parent_tags = doc.findall('.//dflt:note/..',ns)\n",
    "\n",
    "    for parent_tag in notes_parent_tags:\n",
    "\n",
    "        for note_tag in parent_tag.findall('./dflt:note',ns):\n",
    "            parent_tag.remove(note_tag)\n",
    "\n",
    "    # join free text pieces\n",
    "    for f_tag in tag_list[lidx:ridx+1]:\n",
    "        free_text += \" \".join(\"\".join(f_tag.itertext()).split()) + \" \"\n",
    "    \n",
    "    # if after all, free text is still \"\" represent document with - to deal with nan values later.\n",
    "    if free_text==\"\":\n",
    "        free_text = \"-\"\n",
    "    \n",
    "    # compute string measures (lexical richness, polarity, token count)\n",
    "    blob = TextBlob(free_text)\n",
    "    lex = LexicalRichness(free_text)\n",
    "    txt_len = lex.words\n",
    "    subj = round(blob.sentiment[1],2)\n",
    "    pol = round(blob.sentiment[0],2)\n",
    "    ttr = round(lex.ttr,2)\n",
    "    cttr = round(lex.cttr,2)\n",
    "\n",
    "    doc_dict = {'id_to_text':id_to_text,'volume':volume,'subtype':subtype,\n",
    "                    'date':date,'year':year,'title':title,\n",
    "                    'source':source,'person_sentby':person_sentby,'person_sentto':person_sentto,\n",
    "                    'city':city,'era':era,'inst_sentby':inst_sentby,\n",
    "                    'inst_sentto':inst_sentto,'person_mentioned':person_mentioned,\n",
    "                    'inst_mentioned':instution_mentioned,'text':free_text,\n",
    "                    'txt_len':txt_len,'subj':subj,'pol':pol,'ttr':ttr,'cttr':cttr,\n",
    "                    }\n",
    "\n",
    "    \n",
    "    return (person_sentby_dict_list, person_sentto_dict_list, person_mentioned_dict_list, institution_mentioned_dict_list ,doc_dict)\n",
    "\n",
    "\n",
    "# city lookup table for unification\n",
    "with open(tables_path+'city_lookup_dict.json', 'r') as f:\n",
    "    city_lookup_dict = json.load(f)\n",
    "\n",
    "# person id to unified name lookup table\n",
    "new_unified_person_df = pd.read_parquet(tables_path+'new_unified_person_df_final.parquet')\n",
    "\n",
    "person_id_lookup_dict = {} # 'id':'corrected'\n",
    "for _, row in new_unified_person_df.iterrows():\n",
    "\n",
    "    for id in row['id_list']:\n",
    "        if id not in person_id_lookup_dict:\n",
    "            person_id_lookup_dict[id] = row['name_set']\n",
    "\n",
    "\n",
    "# term id to unified name lookup table\n",
    "new_unified_institution_df = pd.read_parquet(tables_path+'new_unified_institution_df.parquet')\n",
    "\n",
    "institution_id_lookup_dict = {} # 'id':'corrected'\n",
    "for _, row in new_unified_institution_df.iterrows():\n",
    "\n",
    "    for id in row['id_list']:\n",
    "        if id not in institution_id_lookup_dict:\n",
    "            institution_id_lookup_dict[id] = row['description_set']\n",
    "\n",
    "# defining useful tag lists for free text's extraction\n",
    "not_text_tags = ['{http://www.tei-c.org/ns/1.0}head',\n",
    "                '{http://www.tei-c.org/ns/1.0}opener',\n",
    "                '{http://www.tei-c.org/ns/1.0}dateline',\n",
    "                '{http://www.tei-c.org/ns/1.0}note',\n",
    "                '{http://www.tei-c.org/ns/1.0}table',]\n",
    "text_tags = ['{http://www.tei-c.org/ns/1.0}p',\n",
    "            '{http://www.tei-c.org/ns/1.0}list']\n",
    "\n",
    "era_df = pd.read_csv('tables/era.csv')\n",
    "era_df['startDate'] = era_df['startDate'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "era_df['endDate'] = era_df['endDate'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "\n",
    "#doc_df = pd.DataFrame(columns=['id_to_text','volume','subtype','date','year','title','source','person_sentby',\n",
    "#                                  'person_sentto','city','era','inst_sentby','inst_sentto',\n",
    "#                                  'person_mentioned','inst_mentioned','text',\n",
    "#                                  'txt_len','subj','pol','ttr','cttr'])\n",
    "\n",
    "#person_sentby_df = pd.DataFrame(columns=['person_name','sent'])\n",
    "#person_sentto_df = pd.DataFrame(columns=['person_name','received'])\n",
    "#person_mentioned_df = pd.DataFrame(columns=['person_name','mentioned_in'])\n",
    "\n",
    "#instution_mentioned_df = pd.DataFrame(columns=['description_set','mentioned_in'])\n",
    "\n",
    "global_person_sentby_list = []\n",
    "global_person_sentto_list = []\n",
    "global_person_mentioned_list = []\n",
    "global_institution_mentioned_list = []\n",
    "global_doc_list = []\n",
    "\n",
    "# only use documents within this years\n",
    "start_year, end_year = 1980, 1988\n",
    "\n",
    "\n",
    "for file in tqdm(glob.glob('volumes/frus*')):\n",
    "    file_start_year = int(file[12:16])\n",
    "\n",
    "    if file_start_year >= start_year and file_start_year<=end_year:\n",
    "        volume = file[8:-4]\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        docs = root.findall('./dflt:text/dflt:body//dflt:div[@type=\"document\"]', ns)\n",
    "        #for doc in docs:\n",
    "        #    #person_sentby_dict_list, person_sentto_dict_list, person_mentioned_dict_list, institution_mentioned_dict_list ,doc_dict = extract_document(doc, volume)\n",
    "        #    result_tuple = extract_document(doc, volume)\n",
    "\n",
    "        futures = [extract_document.remote(doc,volume) for doc in docs]\n",
    "        result_tuple_list = ray.get(futures)\n",
    "\n",
    "        global_person_sentby_list += sum(list(map(lambda x: x[0],result_tuple_list)),[])\n",
    "        global_person_sentto_list += sum(list(map(lambda x: x[1],result_tuple_list)),[])\n",
    "        global_person_mentioned_list += sum(list(map(lambda x: x[2],result_tuple_list)),[])\n",
    "        global_institution_mentioned_list += sum(list(map(lambda x: x[3],result_tuple_list)),[])\n",
    "        global_doc_list += list(map(lambda x: x[4],result_tuple_list))\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "doc_df = pd.DataFrame(global_doc_list)\n",
    "person_sentby_df = pd.DataFrame(global_person_sentby_list)\n",
    "person_sentto_df = pd.DataFrame(global_person_sentto_list)\n",
    "person_mentioned_df = pd.DataFrame(global_person_mentioned_list)\n",
    "instution_mentioned_df = pd.DataFrame(global_institution_mentioned_list)\n",
    "\n",
    "doc_df.to_csv(tables_path+'doc.csv')\n",
    "person_sentby_df.to_csv(tables_path+'person_sentby.csv')\n",
    "person_sentto_df.to_csv(tables_path+'person_sentto.csv')\n",
    "\n",
    "person_mentioned_df = person_mentioned_df[['person_name','mentioned_in']].drop_duplicates().reset_index(drop=True)\n",
    "person_mentioned_df.to_csv(tables_path+'person_mentioned.csv')\n",
    "\n",
    "instution_mentioned_df = instution_mentioned_df[['description_set','mentioned_in']].drop_duplicates().reset_index(drop=True)\n",
    "instution_mentioned_df.to_csv(tables_path+'instution_mentioned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_set</th>\n",
       "      <th>mentioned_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>centers control disease for</td>\n",
       "      <td>frus1981-88v41_d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acquired deficiency immune syndrome</td>\n",
       "      <td>frus1981-88v41_d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acquired deficiency immune syndrome</td>\n",
       "      <td>frus1981-88v41_d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bureau department management, medical of of of...</td>\n",
       "      <td>frus1981-88v41_d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>centers control disease for</td>\n",
       "      <td>frus1981-88v41_d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13304</th>\n",
       "      <td>broadcast foreign information service</td>\n",
       "      <td>frus1981-88v04_d382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>affairs, affairs, and bureau canadian departme...</td>\n",
       "      <td>frus1981-88v04_d382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>states united</td>\n",
       "      <td>frus1981-88v04_d382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>agency central intelligence</td>\n",
       "      <td>frus1981-88v04_d382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>communist of party soviet the union</td>\n",
       "      <td>frus1981-88v04_d382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13309 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description_set         mentioned_in\n",
       "0                            centers control disease for    frus1981-88v41_d1\n",
       "1                    acquired deficiency immune syndrome    frus1981-88v41_d1\n",
       "2                    acquired deficiency immune syndrome    frus1981-88v41_d2\n",
       "3      bureau department management, medical of of of...    frus1981-88v41_d2\n",
       "4                            centers control disease for    frus1981-88v41_d2\n",
       "...                                                  ...                  ...\n",
       "13304              broadcast foreign information service  frus1981-88v04_d382\n",
       "13305  affairs, affairs, and bureau canadian departme...  frus1981-88v04_d382\n",
       "13306                                      states united  frus1981-88v04_d382\n",
       "13307                        agency central intelligence  frus1981-88v04_d382\n",
       "13308                communist of party soviet the union  frus1981-88v04_d382\n",
       "\n",
       "[13309 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instution_mentioned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"COUNTRY_MENTIONED\")\n",
    "\n",
    "ne_df = pd.read_csv('tables/columbia_ner_annotations.csv')\n",
    "country_df = pd.read_csv('tables/tables_52_88/country.csv')\n",
    "doc_df = pd.read_csv('tables/tables_52_88/doc.csv')\n",
    "\n",
    "filtered_ne_df = ne_df[ne_df['itemLabel'].apply(lambda x: x in country_df['countryLabel'].values)]\n",
    "\n",
    "# helper\n",
    "def reformat_file_name(temp_str):\n",
    "    temp_str = temp_str[:-8]\n",
    "\n",
    "    d_index = temp_str.rfind('d')\n",
    "    \n",
    "    return temp_str[:d_index] + '_' + temp_str[d_index:]\n",
    "\n",
    "\n",
    "country_mentioned_df = filtered_ne_df[['file','itemLabel']].drop_duplicates()\n",
    "country_mentioned_df['file'] = country_mentioned_df['file'].apply(lambda x: reformat_file_name(x))\n",
    "country_mentioned_df.rename(columns={'file':'id_to_text','itemLabel':'countryLabel'},inplace=True)\n",
    "\n",
    "# only for part of data (69-76). not needed when whole data.\n",
    "country_mentioned_df = country_mentioned_df[country_mentioned_df['id_to_text'].apply(lambda x: x in doc_df['id_to_text'].values)]\n",
    "\n",
    "country_mentioned_df.reset_index(inplace=True,drop=True)\n",
    "country_mentioned_df.to_csv('tables/tables_52_88/country_mentioned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{'a':1,'b':2},{'a':3,'b':4}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
