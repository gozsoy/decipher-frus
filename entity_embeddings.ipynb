{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gokberk/miniconda3/envs/ml4hc_project2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/gokberk/miniconda3/envs/ml4hc_project2/lib/python3.9/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from graphdatascience import GraphDataScience\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ray\n",
    "ray.init(num_cpus=12)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ner(sentence):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    named_entities = []\n",
    "\n",
    "    entity_chunk = None #(entity,type)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.ent_iob_ == 'O':\n",
    "            if entity_chunk:\n",
    "                named_entities.append(entity_chunk)\n",
    "                entity_chunk = None\n",
    "        elif token.ent_iob_ == 'B':\n",
    "            if entity_chunk:\n",
    "                named_entities.append(entity_chunk)\n",
    "                entity_chunk = None\n",
    "            entity_chunk = (token.text,token.ent_type_)\n",
    "        else:\n",
    "            entity_chunk_text = entity_chunk[0]\n",
    "            entity_chunk_type = entity_chunk[1]\n",
    "            entity_chunk = (entity_chunk_text+' '+token.text,entity_chunk_type)\n",
    "\n",
    "\n",
    "    uninformative_entities = ['DATE','TIME','QUANTITY','ORDINAL','CARDINAL','MONEY','PERCENT','PERSON']\n",
    "\n",
    "    named_entities = list(filter(lambda x: True if x[1] not in uninformative_entities else False, named_entities))\n",
    "    named_entities = np.unique(named_entities,axis=0) \n",
    "\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_path = 'tables/tables_52_88/'\n",
    "\n",
    "doc_df = pd.read_csv(tables_path+'doc.csv')\n",
    "doc_df = doc_df[doc_df['subtype']!='editorial-note'] # removing editorial notes\n",
    "\n",
    "id_to_text_list = doc_df['id_to_text'].values\n",
    "free_text_list = doc_df['text'].values\n",
    "year_list = list(map(lambda x: str(int(x)),doc_df['year'].values))\n",
    "era_list = doc_df['era'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81930/81930 [2:07:35<00:00, 10.70it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne2doc_df computed and saved.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(tables_path+'ne2doc_original.parquet'):\n",
    "    ne2doc_df = pd.read_parquet(tables_path+'ne2doc_original.parquet')\n",
    "    print('ne2doc_df loaded.')\n",
    "\n",
    "else:\n",
    "    ner_dict = {'id_to_text':[], 'named_entity':[], 'year':[], 'era':[]}\n",
    "\n",
    "    for idx,text in enumerate(tqdm(free_text_list)):\n",
    "\n",
    "        if not(isinstance(text, float) and math.isnan(text)): # check if NaN\n",
    "            id_to_text = id_to_text_list[idx]\n",
    "            year = year_list[idx]\n",
    "            era = era_list[idx]\n",
    "            ne_list = apply_ner(text)\n",
    "\n",
    "            for ne_tuple in ne_list:\n",
    "                ne = ne_tuple[0]\n",
    "\n",
    "                ner_dict['id_to_text'].append(id_to_text)\n",
    "                ner_dict['named_entity'].append(ne)\n",
    "                ner_dict['year'].append(int(year))\n",
    "                ner_dict['era'].append(era)\n",
    "    \n",
    "    ne2doc_df = pd.DataFrame(data=ner_dict)\n",
    "    ne2doc_df.to_parquet(tables_path+'ne2doc_original.parquet')\n",
    "    print('ne2doc_df computed and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold based on count \n",
    "min_ne_count = 50\n",
    "ne2doc_df = ne2doc_df.groupby('named_entity').filter(lambda x: len(x) >= min_ne_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x year bins\n",
    "name_extension = '_4yearbinned'\n",
    "bins = list(range(1950,1990,4))\n",
    "\n",
    "labels = []\n",
    "for i in range(1,len(bins)):\n",
    "    labels.append(str(bins[i-1])[-2:]+'-'+str(bins[i])[-2:])\n",
    "\n",
    "ne2doc_df['bin'] = pd.cut(ne2doc_df['year'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "ne2doc_df['dynamic_named_entity'] = ne2doc_df['named_entity'].astype(str) + ' ' + ne2doc_df['bin'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Turkey 54-58    679\n",
       "Turkey 50-54    542\n",
       "Turkey 58-62    448\n",
       "Turkey 70-74    360\n",
       "Turkey 74-78    346\n",
       "Turkey 62-66    341\n",
       "Turkey 66-70    336\n",
       "Turkey 78-82    178\n",
       "Turkey 82-86     15\n",
       "Turkey nan       12\n",
       "Name: dynamic_named_entity, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne2doc_df[ne2doc_df['named_entity']=='Turkey']['dynamic_named_entity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne2doc_df.to_parquet(tables_path+'ne2doc'+name_extension+'.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### now:\n",
    "##### 1- execute python3 ne_conversion.py\n",
    "##### 2- run cypher commands in \"ne2vec/cypher_commands.txt\" on database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = GraphDataScience(\"bolt://localhost:7687\", auth=('neo4j', 'bos'), database='entity2vec18mar')\n",
    "\n",
    "embedding_df = gds.run_cypher(\n",
    "    \"\"\"\n",
    "        match (e:Entity)\n",
    "        return e.name as entity, e['fastrp-embedding'] as fastrp_embedding\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_emb_mat = TSNE(n_components=2, perplexity=50).fit_transform(np.stack(embedding_df['fastrp_embedding']))\n",
    "\n",
    "x,y = reduced_emb_mat[:,0],reduced_emb_mat[:,1]\n",
    "\n",
    "fig = px.scatter(x=x, y=y, text=embedding_df['entity'].values, width=900, height=900)\n",
    "fig.write_html(\"ne2vec/69_76_dynamic_mincnt20_fastrp128.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim_mat = cosine_similarity(np.stack(embedding_df['fastrp_embedding']))\n",
    "\n",
    "def most_similar(word, top_n):\n",
    "\n",
    "    word_idx = embedding_df[embedding_df['entity']==word].index[0]\n",
    "\n",
    "    similar_entity_idx = np.argsort(cossim_mat[word_idx])[::-1][1:top_n+1]\n",
    "\n",
    "    similar_entity_names = embedding_df['entity'].values[similar_entity_idx]\n",
    "    similar_entity_sims = cossim_mat[word_idx][similar_entity_idx]\n",
    "\n",
    "    return np.array([similar_entity_names,similar_entity_sims]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('DEMIREL 74-76',10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
