{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus':'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi':'http://www.w3.org/2001/XInclude'\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = 'frus1969-76v30.xml'\n",
    "tree = ET.parse('volumes/frus1969-76v30.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year: frus:doc-dateTime-max\n",
    "# sent from: <placeName>New York</placeName>\n",
    "# volume from file name\n",
    "# from: <gloss type=\"from\"> or <persName type=\"from\">\n",
    "# to: <gloss type=\"to\"> or <persName type=\"to\">\n",
    "# source: <... type=\"source\" ..>\n",
    "\n",
    "# ignore all <note/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO THIS ANYMORE AS UNIFY_PERSON DOES THE JOB!!!\n",
    "# ENTITY(\"PERSON\")\n",
    "\n",
    "person_df = pd.DataFrame(columns=['name','name_list','id_list'])\n",
    "\n",
    "def extract_person(person_item):\n",
    "    persName_item = person_item.find('.//dflt:persName', ns)\n",
    "    person_name = persName_item.text\n",
    "    person_id = persName_item.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "    all_text = \"\".join(person_item.itertext())\n",
    "    end_idx = all_text.find(person_name) + len(person_name+',')\n",
    "    person_descp = \" \".join(all_text[end_idx:].split())\n",
    "\n",
    "    person_name = \" \".join(re.sub(',','',\" \".join(person_name.split(', ')[::-1])).split())\n",
    "    \n",
    "    corrected_person_name = person_lookup_dict[person_name]\n",
    "    temp_person_df = new_unified_person_df[new_unified_person_df['name_set']==corrected_person_name]\n",
    "    person_name_list = temp_person_df['name_list'].values[0]\n",
    "    person_id_list = temp_person_df['id_list'].values[0]\n",
    "    person_descp_list = temp_person_df['description_list'].values[0]\n",
    "\n",
    "    #person_id = volume[:-4] + '_' + person_id\n",
    "\n",
    "    global person_df\n",
    "    person_df = pd.concat((person_df, pd.DataFrame({'name':[corrected_person_name],\n",
    "                                                    'name_list':[person_name_list],\n",
    "                                                    'id_list':[person_id_list],})),ignore_index=True)\n",
    "    return\n",
    "\n",
    "# create a lookup dict for unifying all person names\n",
    "new_unified_person_df = pd.read_parquet('tables/new_unified_person_df_wikicol.parquet')\n",
    "\n",
    "person_lookup_dict = {} # 'misspelled':'corrected'\n",
    "for _, row in new_unified_person_df.iterrows():\n",
    "\n",
    "    for misspelled_name in row['name_list']:\n",
    "        if misspelled_name not in person_lookup_dict:\n",
    "            person_lookup_dict[misspelled_name] = row['name_set']\n",
    "\n",
    "\n",
    "persons_section = root.find(\"./dflt:text/dflt:front//dflt:div[@xml:id='persons']\", ns)\n",
    "for item in persons_section.findall('.//dflt:item', ns):\n",
    "    extract_person(item)\n",
    "\n",
    "person_df.to_parquet('tables/person_single_volume.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"INSTUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"YEAR\")\n",
    "\n",
    "year_df = pd.DataFrame({'year':np.arange(1861,1982)})\n",
    "year_df.to_csv('tables/year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "era_df = pd.read_csv('tables/era.csv')\n",
    "era_df['startDate'] = era_df['startDate'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "era_df['endDate'] = era_df['endDate'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"DOCUMENT\")\n",
    "# ENTITY(\"PERSON_SENTBY\")\n",
    "# ENTITY(\"PERSON_SENTTO\")\n",
    "# ENTITY(\"PERSON_MENTIONED\")\n",
    "# ENTITY(\"INST_SENTBY\")\n",
    "# ENTITY(\"INST_SENTTO\")\n",
    "# ENTITY(\"INST_MENTIONED\")\n",
    "\n",
    "def extract_document(doc):\n",
    "\n",
    "    global doc_df\n",
    "\n",
    "    global person_sentby_df\n",
    "    global person_sentto_df\n",
    "    global person_mentioned_df\n",
    "\n",
    "    #global instution_sentby_df\n",
    "    #global instution_sentto_df\n",
    "    global instution_mentioned_df\n",
    "\n",
    "    # id\n",
    "    id = volume[:-4] + '_' + doc.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "    # subtype\n",
    "    subtype = doc.attrib['subtype']\n",
    "\n",
    "    # date and year and era\n",
    "    date = None\n",
    "    year = None\n",
    "    era = None\n",
    "    if subtype!='editorial-note':\n",
    "        fmt = doc.attrib['{http://history.state.gov/frus/ns/1.0}doc-dateTime-max']\n",
    "        date = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d')\n",
    "        year = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d').year\n",
    "        era = era_df[(era_df['startDate'] <= date) & (era_df['endDate'] > date)].president.values[0]\n",
    "\n",
    "    # source\n",
    "    source_tag = doc.find('.//dflt:note[@type=\"source\"]',ns)\n",
    "    if source_tag is not None:\n",
    "        source = \" \".join(ET.tostring(source_tag, encoding='unicode', method='text').split())\n",
    "    else:\n",
    "        source = None\n",
    "\n",
    "    # title -includes removing note tag!\n",
    "    head_tag = doc.find('./dflt:head', ns)\n",
    "    child_note_tags = head_tag.findall('./dflt:note', ns)\n",
    "\n",
    "    for note_tag in child_note_tags:\n",
    "        head_tag.remove(note_tag)\n",
    "\n",
    "    title = \" \".join(ET.tostring(head_tag, encoding='unicode', method='text').split())\n",
    "\n",
    "    # city\n",
    "    place_tag = doc.find('.//dflt:placeName',ns)\n",
    "    if place_tag is not None:\n",
    "        txt = \"\".join(place_tag.itertext())\n",
    "        txt = \" \".join(txt.split())\n",
    "        txt = \" \".join(txt.split(',')[0].split())\n",
    "        city = city_lookup_dict[txt]\n",
    "        if city!=txt:\n",
    "            print(txt,city)\n",
    "    else:\n",
    "        city = None\n",
    "\n",
    "    # person_sentby\n",
    "    person_sentby = []\n",
    "\n",
    "    for pers_tag in doc.findall('.//dflt:persName[@type=\"from\"]',ns):\n",
    "        if pers_tag is not None:\n",
    "            if 'corresp' in pers_tag.attrib:\n",
    "                person_id = pers_tag.attrib['corresp'][1:]\n",
    "                person_id = volume[:-4] + '_' + person_id\n",
    "                person_name = person_id_lookup_dict[person_id]\n",
    "                person_sentby.append(person_name)\n",
    "                person_sentby_df = pd.concat((person_sentby_df, \n",
    "                                            pd.DataFrame({'person_name':[person_name],'sent':[id]})),\n",
    "                                            ignore_index=True)\n",
    "            else:\n",
    "                txt = (\" \".join(pers_tag.itertext()))\n",
    "                txt = \" \".join(txt.split())\n",
    "                person_sentby.append(txt)\n",
    "\n",
    "    #docs[0].findall('.//dflt:list',ns)[0].attrib #list -not included yet-\n",
    "\n",
    "    signed_person_tag = doc.find('.//dflt:signed//dflt:persName',ns)\n",
    "    if signed_person_tag is not None:\n",
    "        if 'corresp' in signed_person_tag.attrib:\n",
    "            person_id = signed_person_tag.attrib['corresp'][1:]\n",
    "            person_id = volume[:-4] + '_' + person_id\n",
    "            person_name = person_id_lookup_dict[person_id]\n",
    "            person_sentby.append(person_name)\n",
    "            person_sentby_df = pd.concat((person_sentby_df, \n",
    "                                        pd.DataFrame({'person_name':[person_name],'sent':[id]})),\n",
    "                                        ignore_index=True)\n",
    "        else:\n",
    "            txt = (\" \".join(signed_person_tag.itertext()))\n",
    "            txt = \" \".join(txt.split())\n",
    "            person_sentby.append(txt)\n",
    "\n",
    "    # person_sentto\n",
    "    person_sentto = []\n",
    "\n",
    "    for pers_tag in doc.findall('.//dflt:persName[@type=\"to\"]',ns):\n",
    "        if pers_tag is not None:\n",
    "            if 'corresp' in pers_tag.attrib:\n",
    "                person_id = pers_tag.attrib['corresp'][1:]\n",
    "                person_id = volume[:-4] + '_' + person_id\n",
    "                person_name = person_id_lookup_dict[person_id]\n",
    "                person_sentto.append(person_name)\n",
    "                person_sentto_df = pd.concat((person_sentto_df, \n",
    "                                            pd.DataFrame({'person_name':[person_name],'received':[id]})),\n",
    "                                            ignore_index=True)\n",
    "            else:\n",
    "                txt = (\" \".join(pers_tag.itertext()))\n",
    "                txt = \" \".join(txt.split())\n",
    "                person_sentto.append(txt)\n",
    "\n",
    "    #docs[0].findall('.//dflt:list[@type=\"to\"]',ns)[0].attrib # list -not included yet-\n",
    "\n",
    "\n",
    "    # inst_sentby\n",
    "    inst_sentby = []\n",
    "\n",
    "    for gloss_tag in doc.findall('.//dflt:gloss[@type=\"from\"]',ns):\n",
    "\n",
    "        txt = (\" \".join(gloss_tag.itertext()))\n",
    "        txt = \" \".join(txt.split())\n",
    "        inst_sentby.append(txt)\n",
    "\n",
    "    # inst_sentto\n",
    "    inst_sentto = []\n",
    "\n",
    "    for gloss_tag in doc.findall('.//dflt:gloss[@type=\"to\"]',ns):\n",
    "\n",
    "        txt = (\" \".join(gloss_tag.itertext()))\n",
    "        txt = \" \".join(txt.split())\n",
    "        inst_sentto.append(txt)\n",
    "\n",
    "\n",
    "    # person_mentioned -includes removing note tag!\n",
    "    person_mentioned = set()\n",
    "\n",
    "    notes_parent_tags = doc.findall('.//dflt:note/..',ns)\n",
    "\n",
    "    for parent_tag in notes_parent_tags:\n",
    "\n",
    "        for note_tag in parent_tag.findall('./dflt:note',ns):\n",
    "            parent_tag.remove(note_tag)\n",
    "\n",
    "\n",
    "    pers_tags = doc.findall('.//dflt:persName[@corresp]',ns)\n",
    "    for temp_tag in pers_tags:\n",
    "        person_id = temp_tag.attrib['corresp'][1:]\n",
    "        person_id = volume[:-4] + '_' + person_id\n",
    "        person_name = person_id_lookup_dict[person_id]\n",
    "        person_mentioned.add(person_name)\n",
    "        person_mentioned_df = pd.concat((person_mentioned_df, \n",
    "                                    pd.DataFrame({'person_name':[person_name],'mentioned_in':[id]})),\n",
    "                                    ignore_index=True)\n",
    "\n",
    "\n",
    "    # inst_mentioned -includes removing note tag!\n",
    "    instution_mentioned = set()\n",
    "\n",
    "    inst_tags = doc.findall('.//dflt:gloss[@target]',ns)\n",
    "    for temp_tag in inst_tags:\n",
    "        inst_id = temp_tag.attrib['target'][1:]\n",
    "        instution_mentioned.add(inst_id)\n",
    "        instution_mentioned_df = pd.concat((instution_mentioned_df, \n",
    "                                    pd.DataFrame({'instution_id':[inst_id],'mentioned_in':[id]})),\n",
    "                                    ignore_index=True)\n",
    "\n",
    "    doc_df = pd.concat((doc_df, pd.DataFrame({'id':[id],'volume':[volume[:-4]],'subtype':[subtype],\n",
    "                                             'date':[date],'year':[year],'title':[title],\n",
    "                                             'source':[source],'person_sentby':[person_sentby],'person_sentto':[person_sentto],\n",
    "                                             'city':[city],'era':[era],'inst_sentby':[inst_sentby],\n",
    "                                             'inst_sentto':[inst_sentto],'person_mentioned':[person_mentioned],\n",
    "                                             'inst_mentioned':[instution_mentioned]\n",
    "                                                })),ignore_index=True)\n",
    "    \n",
    "    return\n",
    "\n",
    "# city lookup table for unification\n",
    "with open('tables/city_lookup_dict.json', 'r') as f:\n",
    "    city_lookup_dict = json.load(f)\n",
    "\n",
    "# person id to unified name lookup table\n",
    "new_unified_person_df = pd.read_parquet('tables/new_unified_person_df_wikicol.parquet')\n",
    "\n",
    "person_id_lookup_dict = {} # 'id':'corrected'\n",
    "for _, row in new_unified_person_df.iterrows():\n",
    "\n",
    "    for id in row['id_list']:\n",
    "        if id not in person_id_lookup_dict:\n",
    "            person_id_lookup_dict[id] = row['name_set']\n",
    "\n",
    "\n",
    "doc_df = pd.DataFrame(columns=['id','volume','subtype','date','year','title','source','person_sentby',\n",
    "                                  'person_sentto','city','era','inst_sentby','inst_sentto',\n",
    "                                  'person_mentioned','inst_mentioned'])\n",
    "\n",
    "person_sentby_df = pd.DataFrame(columns=['person_name','sent'])\n",
    "person_sentto_df = pd.DataFrame(columns=['person_name','received'])\n",
    "person_mentioned_df = pd.DataFrame(columns=['person_name','mentioned_in'])\n",
    "\n",
    "#instution_sentby_df = pd.DataFrame(columns=['instution_id','sent'])\n",
    "#instution_sentto_df = pd.DataFrame(columns=['instution_id','received'])\n",
    "instution_mentioned_df = pd.DataFrame(columns=['instution_id','mentioned_in'])\n",
    "\n",
    "\n",
    "docs = root.findall('./dflt:text/dflt:body//dflt:div[@type=\"document\"]', ns)\n",
    "for doc in docs:\n",
    "    extract_document(doc) #(doc,volume)\n",
    "\n",
    "doc_df.to_csv('tables/doc_single_volume.csv')\n",
    "person_sentby_df.to_csv('tables/person_sentby_single_volume.csv')\n",
    "person_sentto_df.to_csv('tables/person_sentto_single_volume.csv')\n",
    "person_mentioned_df.to_csv('tables/person_mentioned_single_volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"COUNTRY\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
