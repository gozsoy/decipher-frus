{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus':'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi':'http://www.w3.org/2001/XInclude'\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = 'frus1969-76v30.xml'\n",
    "tree = ET.parse('volumes/frus1969-76v30.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year: frus:doc-dateTime-max\n",
    "# sent from: <placeName>New York</placeName>\n",
    "# volume from file name\n",
    "# from: <gloss type=\"from\"> or <persName type=\"from\">\n",
    "# to: <gloss type=\"to\"> or <persName type=\"to\">\n",
    "# source: <... type=\"source\" ..>\n",
    "\n",
    "# ignore all <note/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"PERSON\")\n",
    "\n",
    "person_df = pd.DataFrame(columns=['id','name','description'])\n",
    "\n",
    "def extract_person(person_item):\n",
    "    persName_item = person_item.find('.//dflt:persName', ns)\n",
    "    person_name = persName_item.text\n",
    "    person_id = persName_item.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "    all_text = \"\".join(person_item.itertext())\n",
    "    end_idx = all_text.find(person_name) + len(person_name+',')\n",
    "    person_descp = \" \".join(all_text[end_idx:].split())\n",
    "\n",
    "    person_name = \" \".join(re.sub(',','',\" \".join(person_name.split(', ')[::-1])).split())\n",
    "\n",
    "    #person_id = volume[:-4] + '_' + person_id\n",
    "\n",
    "    global person_df\n",
    "    person_df = pd.concat((person_df, pd.DataFrame({'id':[person_id],\n",
    "                                                'name':[person_name],\n",
    "                                                'description':[person_descp]})),ignore_index=True)\n",
    "    return\n",
    "\n",
    "\n",
    "persons_section = root.find(\"./dflt:text/dflt:front//dflt:div[@xml:id='persons']\", ns)\n",
    "for item in persons_section.findall('.//dflt:item', ns):\n",
    "    extract_person(item)\n",
    "\n",
    "person_df.to_csv('tables/person_single_volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"YEAR\")\n",
    "\n",
    "year_df = pd.DataFrame({'year':np.arange(1861,1982)})\n",
    "year_df.to_csv('tables/year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"DOCUMENT\")\n",
    "\n",
    "person_df = pd.DataFrame(columns=['id','volume','subtype','date','year','title','source','person_sentby',\n",
    "                                  'person_sentto','city','era','inst_sentby','inst_sentto',\n",
    "                                  'person_mentioned','inst_mentioned'])\n",
    "\n",
    "docs = root.findall('./dflt:text/dflt:body//dflt:div[@type=\"document\"]', ns)\n",
    "\n",
    "# extract head as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973-03-30 00:00:00\n",
      "1973\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# id\n",
    "id = volume[:-4] + '_' + docs[0].attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "# subtype\n",
    "subtype = docs[0].attrib['subtype']\n",
    "\n",
    "# date and year\n",
    "if subtype!='editorial-note':\n",
    "    fmt = docs[0].attrib['{http://history.state.gov/frus/ns/1.0}doc-dateTime-max']\n",
    "    date = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d')\n",
    "    year = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d').year\n",
    "\n",
    "# source\n",
    "source_tag = docs[0].find('.//dflt:note[@type=\"source\"]',ns)\n",
    "source = \" \".join(ET.tostring(source_tag, encoding='unicode', method='text').split())\n",
    "\n",
    "# title\n",
    "head_tag = docs[0].find('./dflt:head', ns)\n",
    "child_note_tags = head_tag.findall('./dflt:note', ns)\n",
    "\n",
    "for note_tag in child_note_tags:\n",
    "    head_tag.remove(note_tag)\n",
    "\n",
    "title = \" \".join(ET.tostring(head_tag, encoding='unicode', method='text').split())\n",
    "\n",
    "# city\n",
    "city = docs[0].find('.//dflt:placeName',ns).text\n",
    "\n",
    "# person_sentby\n",
    "person_sentby = []\n",
    "\n",
    "# persName\n",
    "for pers_tag in docs[0].findall('.//dflt:persName[@type=\"from\"]',ns):\n",
    "    if pers_tag is not None:\n",
    "        if 'corresp' in pers_tag.attrib:\n",
    "            person_sentby.append(pers_tag.attrib['corresp'])\n",
    "        else:\n",
    "            person_sentby.append(pers_tag.text)\n",
    "\n",
    "# list -not included yet-\n",
    "#docs[0].findall('.//dflt:list',ns)[0].attrib\n",
    "\n",
    "\n",
    "# signed\n",
    "signed_person_tag = docs[0].find('.//dflt:signed//dflt:persName',ns)\n",
    "if signed_person_tag is not None:\n",
    "    if 'corresp' in signed_person_tag.attrib:\n",
    "        person_sentby.append(signed_person_tag.attrib['corresp'])\n",
    "    else:\n",
    "        person_sentby.append(signed_person_tag.text)\n",
    "\n",
    "# person_sentto\n",
    "person_sentto = []\n",
    "\n",
    "# persName\n",
    "for pers_tag in docs[0].findall('.//dflt:persName[@type=\"to\"]',ns):\n",
    "    if pers_tag is not None:\n",
    "        if 'corresp' in pers_tag.attrib:\n",
    "            person_sentto.append(pers_tag.attrib['corresp'])\n",
    "        else:\n",
    "            person_sentto.append(pers_tag.text)\n",
    "\n",
    "# list -not included yet-\n",
    "#docs[0].findall('.//dflt:list[@type=\"to\"]',ns)[0].attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"ERA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## done above ^\n",
    "## construction below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function handles file's sender and receiver\n",
    "def process_from_to_types(from_list):\n",
    "\n",
    "    processed_list = []\n",
    "\n",
    "    for item in from_list:\n",
    "        \n",
    "        if item.tag == 'persName' and 'corresp' in item.attrib.keys():\n",
    "            person_id = item.attrib['corresp']\n",
    "            processed_list.append(person_id)\n",
    "        elif item.tag == 'persName':\n",
    "            processed_list.append(item.text)\n",
    "        elif item.tag == 'gloss':\n",
    "            processed_list.append(item.text)\n",
    "        else:\n",
    "            raise NotImplementedError('from type tag unidentified')\n",
    "    \n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subseries_id = root.find('teiHeader/fileDesc/titleStmt/title[@type=\"sub-series\"]').text\n",
    "volume_id = root.find('teiHeader/fileDesc/titleStmt/title[@type=\"volume-number\"]').text\n",
    "\n",
    "doc_dict = {}\n",
    "\n",
    "def process_document(doc_elem):\n",
    "\n",
    "    doc_id = doc_elem.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "    doc_date = doc_elem.attrib[\"{http://history.state.gov/frus/ns/1.0}doc-dateTime-max\"].split('T')[0]\n",
    "    doc_date = datetime.strptime(doc_date, '%Y-%m-%d')\n",
    "\n",
    "    doc_sent_place = doc_elem.find('.//placeName')\n",
    "    if doc_sent_place is not None:\n",
    "        doc_sent_place = doc_sent_place.text\n",
    "\n",
    "    from_list = doc_elem.findall('head//*[@type=\"from\"]')\n",
    "    to_list = doc_elem.findall('head//*[@type=\"to\"]')\n",
    "\n",
    "    doc_sent_from = process_from_to_types(from_list)\n",
    "    doc_sent_to = process_from_to_types(to_list)\n",
    "\n",
    "    doc_source = doc_elem.find('.//*[@type=\"source\"]')\n",
    "    if doc_source is not None:\n",
    "        doc_source = \" \".join(\"\".join(doc_source.itertext()).split())\n",
    "\n",
    "    doc_dict[\"-\".join([subseries_id,volume_id,doc_id])] = {'subseries':subseries_id, 'volume':volume_id, 'doc_id':doc_id,'date':doc_date, 'place':doc_sent_place, 'from':doc_sent_from, 'to':doc_sent_to, 'source':doc_source}\n",
    "\n",
    "    return\n",
    "\n",
    "for doc_elem in root.findall('text/body/div//div[@type=\"document\"]'):\n",
    "    process_document(doc_elem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
