{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import ray\n",
    "ray.init(num_cpus=13)\n",
    "\n",
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus':'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi':'http://www.w3.org/2001/XInclude'\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "tables_path = 'tables/tables_52_62/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [07:32<00:00,  1.20it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['description_set', 'mentioned_in'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 331\u001b[0m\n\u001b[1;32m    328\u001b[0m person_mentioned_df \u001b[39m=\u001b[39m person_mentioned_df[[\u001b[39m'\u001b[39m\u001b[39mperson_name\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmentioned_in\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mdrop_duplicates()\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    329\u001b[0m person_mentioned_df\u001b[39m.\u001b[39mto_csv(tables_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mperson_mentioned.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m instution_mentioned_df \u001b[39m=\u001b[39m instution_mentioned_df[[\u001b[39m'\u001b[39;49m\u001b[39mdescription_set\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mmentioned_in\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mdrop_duplicates()\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    332\u001b[0m instution_mentioned_df\u001b[39m.\u001b[39mto_csv(tables_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39minstution_mentioned.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/eth_courses/decipher-frus/unified_env/lib/python3.8/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3810\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/eth_courses/decipher-frus/unified_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6115\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6117\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/eth_courses/decipher-frus/unified_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:6173\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6171\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6172\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6175\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6176\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['description_set', 'mentioned_in'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# ENTITY(\"DOCUMENT\")\n",
    "# ENTITY(\"PERSON_SENTBY\")\n",
    "# ENTITY(\"PERSON_SENTTO\")\n",
    "# ENTITY(\"PERSON_MENTIONED\")\n",
    "# ENTITY(\"INST_MENTIONED\")\n",
    "\n",
    "def extract_document(doc, volume):\n",
    "\n",
    "    #global doc_df\n",
    "\n",
    "    #global person_sentby_df\n",
    "    #global person_sentto_df\n",
    "    #global person_mentioned_df\n",
    "\n",
    "    #global instution_mentioned_df\n",
    "\n",
    "    # id\n",
    "    id_to_text = volume + '_' + doc.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "    # subtype\n",
    "    subtype = doc.attrib['subtype']\n",
    "\n",
    "    # date and year and era\n",
    "    date = None\n",
    "    year = None\n",
    "    era = None\n",
    "    if subtype!='editorial-note':\n",
    "        fmt = doc.attrib['{http://history.state.gov/frus/ns/1.0}doc-dateTime-max']\n",
    "        date = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d')\n",
    "        year = datetime.strptime(fmt.split('T')[0], '%Y-%m-%d').year\n",
    "        era = era_df[(era_df['startDate'] <= date) & (era_df['endDate'] > date)].president.values[0]\n",
    "\n",
    "    # source\n",
    "    source_tag = doc.find('.//dflt:note[@type=\"source\"]',ns)\n",
    "    if source_tag is not None:\n",
    "        source = \" \".join(ET.tostring(source_tag, encoding='unicode', method='text').split())\n",
    "    else:\n",
    "        source = None\n",
    "\n",
    "    # title -includes removing note tag!\n",
    "    head_tag = doc.find('./dflt:head', ns)\n",
    "    child_note_tags = head_tag.findall('./dflt:note', ns)\n",
    "\n",
    "    for note_tag in child_note_tags:\n",
    "        head_tag.remove(note_tag)\n",
    "\n",
    "    title = \" \".join(ET.tostring(head_tag, encoding='unicode', method='text').split())\n",
    "\n",
    "    # city\n",
    "    place_tag = doc.find('.//dflt:placeName',ns)\n",
    "    if place_tag is not None:\n",
    "        txt = \"\".join(place_tag.itertext())\n",
    "        txt = \" \".join(txt.split())\n",
    "        txt = \" \".join(txt.split(',')[0].split())\n",
    "        city = city_lookup_dict[txt]\n",
    "    else:\n",
    "        city = None\n",
    "\n",
    "    # person_sentby\n",
    "    person_sentby = []\n",
    "    person_sentby_dict_list = []\n",
    "\n",
    "    for pers_tag in doc.findall('.//dflt:persName[@type=\"from\"]',ns):\n",
    "        if pers_tag is not None: \n",
    "            if 'corresp' in pers_tag.attrib:\n",
    "                if pers_tag.attrib['corresp'][0]=='#':\n",
    "                    person_id = pers_tag.attrib['corresp'][1:]\n",
    "                else:\n",
    "                    person_id = pers_tag.attrib['corresp']\n",
    "                person_id = volume + '_' + person_id\n",
    "                person_name = person_id_lookup_dict.get(person_id,None)\n",
    "                if person_name:\n",
    "                    person_sentby.append(person_name)\n",
    "                    person_sentby_dict_list.append({'person_name':person_name,'sent':id_to_text})\n",
    "            else:\n",
    "                txt = (\" \".join(pers_tag.itertext()))\n",
    "                txt = \" \".join(txt.split())\n",
    "                person_sentby.append(txt)\n",
    "\n",
    "    #docs[0].findall('.//dflt:list',ns)[0].attrib #list -not included yet-\n",
    "\n",
    "    signed_person_tag = doc.find('.//dflt:signed//dflt:persName',ns)\n",
    "    if signed_person_tag is not None:\n",
    "        if 'corresp' in signed_person_tag.attrib:\n",
    "            person_id = signed_person_tag.attrib['corresp'][1:]\n",
    "            if signed_person_tag.attrib['corresp'][0]=='#':\n",
    "                person_id = signed_person_tag.attrib['corresp'][1:]\n",
    "            else:\n",
    "                person_id = signed_person_tag.attrib['corresp']\n",
    "            person_id = volume + '_' + person_id\n",
    "            person_name = person_id_lookup_dict.get(person_id,None)\n",
    "            if person_name:\n",
    "                person_sentby.append(person_name)\n",
    "                person_sentby_dict_list.append({'person_name':person_name,'sent':id_to_text})\n",
    "        else:\n",
    "            txt = (\" \".join(signed_person_tag.itertext()))\n",
    "            txt = \" \".join(txt.split())\n",
    "            person_sentby.append(txt)\n",
    "\n",
    "    # person_sentto\n",
    "    person_sentto = []\n",
    "    person_sentto_dict_list = []\n",
    "\n",
    "    for pers_tag in doc.findall('.//dflt:persName[@type=\"to\"]',ns):\n",
    "        if pers_tag is not None:\n",
    "            if 'corresp' in pers_tag.attrib:\n",
    "                if pers_tag.attrib['corresp'][0]=='#':\n",
    "                    person_id = pers_tag.attrib['corresp'][1:]\n",
    "                else:\n",
    "                    person_id = pers_tag.attrib['corresp']\n",
    "                person_id = volume + '_' + person_id\n",
    "                person_name = person_id_lookup_dict.get(person_id,None)\n",
    "                if person_name:\n",
    "                    person_sentto.append(person_name)\n",
    "                    person_sentto_dict_list.append({'person_name':person_name,'received':id_to_text})\n",
    "            else:\n",
    "                txt = (\" \".join(pers_tag.itertext()))\n",
    "                txt = \" \".join(txt.split())\n",
    "                person_sentto.append(txt)\n",
    "\n",
    "    #docs[0].findall('.//dflt:list[@type=\"to\"]',ns)[0].attrib # list -not included yet-\n",
    "\n",
    "\n",
    "    # inst_sentby\n",
    "    inst_sentby = []\n",
    "\n",
    "    for gloss_tag in doc.findall('.//dflt:gloss[@type=\"from\"]',ns):\n",
    "\n",
    "        txt = (\" \".join(gloss_tag.itertext()))\n",
    "        txt = \" \".join(txt.split())\n",
    "        inst_sentby.append(txt)\n",
    "\n",
    "    # inst_sentto\n",
    "    inst_sentto = []\n",
    "\n",
    "    for gloss_tag in doc.findall('.//dflt:gloss[@type=\"to\"]',ns):\n",
    "\n",
    "        txt = (\" \".join(gloss_tag.itertext()))\n",
    "        txt = \" \".join(txt.split())\n",
    "        inst_sentto.append(txt)\n",
    "\n",
    "\n",
    "    # person_mentioned -includes removing note tag!\n",
    "    person_mentioned = set()\n",
    "    person_mentioned_dict_list = []\n",
    "\n",
    "    notes_parent_tags = doc.findall('.//dflt:note/..',ns)\n",
    "\n",
    "    for parent_tag in notes_parent_tags:\n",
    "\n",
    "        for note_tag in parent_tag.findall('./dflt:note',ns):\n",
    "            parent_tag.remove(note_tag)\n",
    "\n",
    "\n",
    "    pers_tags = doc.findall('.//dflt:persName[@corresp]',ns)\n",
    "    for temp_tag in pers_tags:\n",
    "        if temp_tag.attrib['corresp'][0]=='#':\n",
    "            person_id = temp_tag.attrib['corresp'][1:]\n",
    "        else:\n",
    "            person_id = temp_tag.attrib['corresp']\n",
    "        person_id = volume + '_' + person_id\n",
    "        person_name = person_id_lookup_dict.get(person_id,None)\n",
    "        if person_name:\n",
    "            person_mentioned.add(person_name)\n",
    "            person_mentioned_dict_list.append({'person_name':person_name,'mentioned_in':id_to_text})\n",
    "\n",
    "\n",
    "    # inst_mentioned -includes removing note tag!\n",
    "    instution_mentioned = set()\n",
    "    institution_mentioned_dict_list = []\n",
    "\n",
    "    inst_tags = doc.findall('.//dflt:gloss[@target]',ns)\n",
    "    for temp_tag in inst_tags:\n",
    "        if temp_tag.attrib['target'][0]=='#':\n",
    "            term_id = temp_tag.attrib['target'][1:]\n",
    "        else:\n",
    "            term_id = temp_tag.attrib['target']\n",
    "        term_id = volume + '_' + term_id\n",
    "        term_description_set = institution_id_lookup_dict.get(term_id,None)\n",
    "        if term_description_set:\n",
    "            instution_mentioned.add(term_description_set)\n",
    "            institution_mentioned_dict_list.append({'description_set':term_description_set,'mentioned_in':id_to_text})\n",
    "\n",
    "\n",
    "    # free text\n",
    "    free_text = \"\"\n",
    "\n",
    "    tag_list = doc.findall('./*',ns)\n",
    "    \n",
    "    # find free text's start and end elements\n",
    "    lidx,ridx = 0,0\n",
    "\n",
    "    for idx,tag in enumerate(tag_list):\n",
    "        if tag.tag not in not_text_tags:\n",
    "            lidx = idx\n",
    "            break\n",
    "    \n",
    "    for idx,tag in enumerate(tag_list[::-1]):\n",
    "        if tag.tag in text_tags:\n",
    "            ridx = len(tag_list)-1-idx\n",
    "            break\n",
    "    \n",
    "    # remove all <note> in free text\n",
    "    notes_parent_tags = doc.findall('.//dflt:note/..',ns)\n",
    "\n",
    "    for parent_tag in notes_parent_tags:\n",
    "\n",
    "        for note_tag in parent_tag.findall('./dflt:note',ns):\n",
    "            parent_tag.remove(note_tag)\n",
    "\n",
    "    # join free text pieces\n",
    "    for f_tag in tag_list[lidx:ridx+1]:\n",
    "        free_text += \" \".join(\"\".join(f_tag.itertext()).split()) + \" \"\n",
    "    \n",
    "    # if after all, free text is still \"\" represent document with - to deal with nan values later.\n",
    "    if free_text==\"\":\n",
    "        free_text = \"-\"\n",
    "    \n",
    "    # compute string measures (lexical richness, polarity, token count)\n",
    "    spacy_doc = nlp(free_text)\n",
    "    lex = LexicalRichness(free_text)\n",
    "    txt_len = len(spacy_doc)\n",
    "    subj = round(spacy_doc._.blob.subjectivity,2)\n",
    "    pol = round(spacy_doc._.blob.polarity,2)\n",
    "    ttr = round(lex.ttr,2)\n",
    "    cttr = round(lex.cttr,2)\n",
    "\n",
    "    doc_dict = {'id_to_text':id_to_text,'volume':volume,'subtype':subtype,\n",
    "                    'date':date,'year':year,'title':title,\n",
    "                    'source':source,'person_sentby':person_sentby,'person_sentto':person_sentto,\n",
    "                    'city':city,'era':era,'inst_sentby':inst_sentby,\n",
    "                    'inst_sentto':inst_sentto,'person_mentioned':person_mentioned,\n",
    "                    'inst_mentioned':instution_mentioned,'text':free_text,\n",
    "                    'txt_len':txt_len,'subj':subj,'pol':pol,'ttr':ttr,'cttr':cttr,\n",
    "                    }\n",
    "\n",
    "    \n",
    "    return person_sentby_dict_list, person_sentto_dict_list, person_mentioned_dict_list, institution_mentioned_dict_list ,doc_dict\n",
    "\n",
    "\n",
    "# city lookup table for unification\n",
    "with open(tables_path+'city_lookup_dict.json', 'r') as f:\n",
    "    city_lookup_dict = json.load(f)\n",
    "\n",
    "# person id to unified name lookup table\n",
    "new_unified_person_df = pd.read_parquet(tables_path+'new_unified_person_df_final.parquet')\n",
    "\n",
    "person_id_lookup_dict = {} # 'id':'corrected'\n",
    "for _, row in new_unified_person_df.iterrows():\n",
    "\n",
    "    for id in row['id_list']:\n",
    "        if id not in person_id_lookup_dict:\n",
    "            person_id_lookup_dict[id] = row['name_set']\n",
    "\n",
    "\n",
    "# term id to unified name lookup table\n",
    "new_unified_institution_df = pd.read_parquet(tables_path+'new_unified_institution_df.parquet')\n",
    "\n",
    "institution_id_lookup_dict = {} # 'id':'corrected'\n",
    "for _, row in new_unified_institution_df.iterrows():\n",
    "\n",
    "    for id in row['id_list']:\n",
    "        if id not in institution_id_lookup_dict:\n",
    "            institution_id_lookup_dict[id] = row['description_set']\n",
    "\n",
    "# defining useful tag lists for free text's extraction\n",
    "not_text_tags = ['{http://www.tei-c.org/ns/1.0}head',\n",
    "                '{http://www.tei-c.org/ns/1.0}opener',\n",
    "                '{http://www.tei-c.org/ns/1.0}dateline',\n",
    "                '{http://www.tei-c.org/ns/1.0}note',\n",
    "                '{http://www.tei-c.org/ns/1.0}table',]\n",
    "text_tags = ['{http://www.tei-c.org/ns/1.0}p',\n",
    "            '{http://www.tei-c.org/ns/1.0}list']\n",
    "\n",
    "era_df = pd.read_csv('tables/era.csv')\n",
    "era_df['startDate'] = era_df['startDate'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "era_df['endDate'] = era_df['endDate'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d'))\n",
    "\n",
    "#doc_df = pd.DataFrame(columns=['id_to_text','volume','subtype','date','year','title','source','person_sentby',\n",
    "#                                  'person_sentto','city','era','inst_sentby','inst_sentto',\n",
    "#                                  'person_mentioned','inst_mentioned','text',\n",
    "#                                  'txt_len','subj','pol','ttr','cttr'])\n",
    "\n",
    "#person_sentby_df = pd.DataFrame(columns=['person_name','sent'])\n",
    "#person_sentto_df = pd.DataFrame(columns=['person_name','received'])\n",
    "#person_mentioned_df = pd.DataFrame(columns=['person_name','mentioned_in'])\n",
    "\n",
    "#instution_mentioned_df = pd.DataFrame(columns=['description_set','mentioned_in'])\n",
    "\n",
    "global_person_sentby_list = []\n",
    "global_person_sentto_list = []\n",
    "global_person_mentioned_list = []\n",
    "global_institution_mentioned_list = []\n",
    "global_doc_list = []\n",
    "\n",
    "# only use documents within this years\n",
    "start_year, end_year = 1980, 1988\n",
    "\n",
    "@ray.remote\n",
    "def extract_volume(file):\n",
    "    file_start_year = int(file[12:16])\n",
    "\n",
    "    if file_start_year >= start_year and file_start_year<=end_year:\n",
    "        volume = file[8:-4]\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        docs = root.findall('./dflt:text/dflt:body//dflt:div[@type=\"document\"]', ns)\n",
    "        for doc in docs:\n",
    "            person_sentby_dict_list, person_sentto_dict_list, person_mentioned_dict_list, institution_mentioned_dict_list ,doc_dict = extract_document(doc, volume)\n",
    "            global_person_sentby_list += person_sentby_dict_list\n",
    "            global_person_sentto_list += person_sentto_dict_list\n",
    "            global_person_mentioned_list += person_mentioned_dict_list\n",
    "            global_institution_mentioned_list += institution_mentioned_dict_list\n",
    "            global_doc_list.append(doc_dict)\n",
    "\n",
    "\n",
    "for file in tqdm(glob.glob('volumes/frus*')):\n",
    "    file_start_year = int(file[12:16])\n",
    "\n",
    "    if file_start_year >= start_year and file_start_year<=end_year:\n",
    "        volume = file[8:-4]\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        docs = root.findall('./dflt:text/dflt:body//dflt:div[@type=\"document\"]', ns)\n",
    "        for doc in docs:\n",
    "            person_sentby_dict_list, person_sentto_dict_list, person_mentioned_dict_list, institution_mentioned_dict_list ,doc_dict = extract_document(doc, volume)\n",
    "            global_person_sentby_list += person_sentby_dict_list\n",
    "            global_person_sentto_list += person_sentto_dict_list\n",
    "            global_person_mentioned_list += person_mentioned_dict_list\n",
    "            global_institution_mentioned_list += institution_mentioned_dict_list\n",
    "            global_doc_list.append(doc_dict)\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "'''doc_df = pd.DataFrame(global_doc_list)\n",
    "person_sentby_df = pd.DataFrame(global_person_sentby_list)\n",
    "person_sentto_df = pd.DataFrame(global_person_sentto_list)\n",
    "person_mentioned_df = pd.DataFrame(global_person_mentioned_list)\n",
    "instution_mentioned_df = pd.DataFrame(global_institution_mentioned_list)\n",
    "\n",
    "doc_df.to_csv(tables_path+'doc.csv')\n",
    "person_sentby_df.to_csv(tables_path+'person_sentby.csv')\n",
    "person_sentto_df.to_csv(tables_path+'person_sentto.csv')\n",
    "\n",
    "person_mentioned_df = person_mentioned_df[['person_name','mentioned_in']].drop_duplicates().reset_index(drop=True)\n",
    "person_mentioned_df.to_csv(tables_path+'person_mentioned.csv')\n",
    "\n",
    "instution_mentioned_df = instution_mentioned_df[['description_set','mentioned_in']].drop_duplicates().reset_index(drop=True)\n",
    "instution_mentioned_df.to_csv(tables_path+'instution_mentioned.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_institution_mentioned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY(\"COUNTRY_MENTIONED\")\n",
    "\n",
    "ne_df = pd.read_csv('tables/columbia_ner_annotations.csv')\n",
    "country_df = pd.read_csv('tables/tables_52_88/country.csv')\n",
    "doc_df = pd.read_csv('tables/tables_52_88/doc.csv')\n",
    "\n",
    "filtered_ne_df = ne_df[ne_df['itemLabel'].apply(lambda x: x in country_df['countryLabel'].values)]\n",
    "\n",
    "# helper\n",
    "def reformat_file_name(temp_str):\n",
    "    temp_str = temp_str[:-8]\n",
    "\n",
    "    d_index = temp_str.rfind('d')\n",
    "    \n",
    "    return temp_str[:d_index] + '_' + temp_str[d_index:]\n",
    "\n",
    "\n",
    "country_mentioned_df = filtered_ne_df[['file','itemLabel']].drop_duplicates()\n",
    "country_mentioned_df['file'] = country_mentioned_df['file'].apply(lambda x: reformat_file_name(x))\n",
    "country_mentioned_df.rename(columns={'file':'id_to_text','itemLabel':'countryLabel'},inplace=True)\n",
    "\n",
    "# only for part of data (69-76). not needed when whole data.\n",
    "country_mentioned_df = country_mentioned_df[country_mentioned_df['id_to_text'].apply(lambda x: x in doc_df['id_to_text'].values)]\n",
    "\n",
    "country_mentioned_df.reset_index(inplace=True,drop=True)\n",
    "country_mentioned_df.to_csv('tables/tables_52_88/country_mentioned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  2\n",
       "1  3  4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([{'a':1,'b':2},{'a':3,'b':4}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc_project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
