{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import glob \n",
    "import math\n",
    "import itertools\n",
    "import jellyfish\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus':'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi':'http://www.w3.org/2001/XInclude'\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person(item, file):\n",
    "    volume = file[8:-4]\n",
    "\n",
    "    persName_item = item.find('.//dflt:persName[@xml:id]', ns)\n",
    "\n",
    "    if persName_item is not None:\n",
    "\n",
    "        persName_text = \"\".join(persName_item.itertext())\n",
    "        person_id = persName_item.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "        all_text = \"\".join(item.itertext())\n",
    "        end_idx = all_text.find(persName_text) + len(persName_text+',')\n",
    "        person_descp = \" \".join(all_text[end_idx:].split())\n",
    "\n",
    "        person_name = \" \".join(re.sub(',','',\" \".join(persName_text.split(', ')[::-1])).split())\n",
    "\n",
    "        person_id = volume + '_' + person_id\n",
    "\n",
    "        global person_df\n",
    "        person_df = pd.concat((person_df, pd.DataFrame({'id':[person_id],\n",
    "                                                    'name':[person_name],\n",
    "                                                    'description':[person_descp]})),ignore_index=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = 1952, 1988\n",
    "\n",
    "person_df = pd.DataFrame(columns=['id','name','description'])\n",
    "\n",
    "for file in glob.glob('volumes/frus*'):\n",
    "    file_start_year = int(file[12:16])\n",
    "    \n",
    "    if file_start_year >= start_year and file_start_year<=end_year:\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        persons_section = root.find(\"./dflt:text/dflt:front//dflt:div[@xml:id='persons']\", ns)\n",
    "        \n",
    "        if persons_section:\n",
    "            for item in persons_section.findall('.//dflt:item/dflt:hi/dflt:persName[@xml:id]/../..', ns):\n",
    "                extract_person(item,file)\n",
    "            for item in persons_section.findall('.//dflt:item/dflt:persName[@xml:id]/..', ns):\n",
    "                extract_person(item,file)\n",
    "        else:\n",
    "            print(f'No person annotation in {file}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: reduce exactly matched names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux(row):\n",
    "    global unified_person_dict\n",
    "\n",
    "    if row['name'] in unified_person_dict:\n",
    "      \n",
    "      temp_dict = unified_person_dict[row['name']]\n",
    "\n",
    "      temp_dict['id_list'].append(row['id'])\n",
    "      temp_dict['description_list'].append(row['description'])\n",
    "    \n",
    "    else:\n",
    "      unified_person_dict[row['name']]= {'id_list':[row['id']],\n",
    "                                        'description_list':[row['description']]}\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_df.apply(lambda x:aux(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_df = pd.DataFrame.from_dict(unified_person_dict,orient='index').reset_index(drop=False)\n",
    "unified_person_df.rename(columns={'index':'name'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: reduce names with exactly same words but different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_df['name_set'] = unified_person_df.name.apply(lambda x: \" \".join(sorted(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_dict = {}\n",
    "\n",
    "def aux2(row):\n",
    "    global new_unified_person_dict\n",
    "\n",
    "    if row['name_set'] in new_unified_person_dict:\n",
    "      \n",
    "        temp_dict = new_unified_person_dict[row['name_set']]\n",
    "\n",
    "        temp_dict['name_list'].append(row['name'])\n",
    "        temp_dict['id_list'] += row['id_list']\n",
    "        temp_dict['description_list'] += row['description_list']\n",
    "    \n",
    "    else:\n",
    "        new_unified_person_dict[row['name_set']]= {'name_list':[row['name']],\n",
    "                                                    'id_list':row['id_list'],\n",
    "                                                    'description_list':row['description_list']}\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_df.apply(lambda x:aux2(x), axis=1)\n",
    "\n",
    "new_unified_person_df = pd.DataFrame.from_dict(new_unified_person_dict,orient='index').reset_index(drop=False)\n",
    "new_unified_person_df.rename(columns={'index':'name_set'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for observation\n",
    "new_unified_person_df[new_unified_person_df['name_list'].apply(lambda x: len(x)==2)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: find and reduce near-duplicate names + obvious misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one: (match len>=2 and each word len>=3), edit distance based matching\n",
    "# step two: find misspelling matches with edit distance of 1 e.g. for Ziegler vs Zeigler\n",
    "\n",
    "# caution!!!\n",
    "# Eliot Jr. L. Theodore, and D. Dwight Eisenhower\n",
    "# Georges Guay R. vs George Guay R.\n",
    "# Abrams Creighton General Major W.\n",
    "# Aharon General Major Yariv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = new_unified_person_df['name_set'].values\n",
    "\n",
    "def compute_sim(s1,func,s2):\n",
    "    return func(s1,s2)\n",
    "\n",
    "def compute_exact_word_overlap(s1,s2):\n",
    "    l1 = set([x for x in list(set(tokenizer.tokenize(s1))) if len(x)>=3])\n",
    "    l2 = set([x for x in list(set(tokenizer.tokenize(s2))) if len(x)>=3])\n",
    "\n",
    "    return len(l1.intersection(l2))\n",
    "\n",
    "def find_matches(s2):\n",
    "\n",
    "    spiro_dist_df = pd.DataFrame({'name_set':all_names,\n",
    "                                'overlap_cnt':[compute_exact_word_overlap(x,s2) for x in all_names],\n",
    "                                'dam_lev_dist':[compute_sim(x, jellyfish.damerau_levenshtein_distance,s2) for x in all_names],\n",
    "                                'jaro_sim':[compute_sim(x, jellyfish.jaro_winkler_similarity,s2) for x in all_names]})\n",
    "    \n",
    "    # addition to original matching criteria\n",
    "    misspelling_idx = set(spiro_dist_df[(spiro_dist_df['dam_lev_dist'] <=1)].index.values)\n",
    "\n",
    "    spiro_dist_df = spiro_dist_df[spiro_dist_df['overlap_cnt']>=2]\n",
    "    match_idx = set(spiro_dist_df[(spiro_dist_df['jaro_sim'] >= 0.9) | (spiro_dist_df['dam_lev_dist'] <=5)].index.values)\n",
    "\n",
    "    return match_idx.union(misspelling_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "for idx in tqdm(range(len(all_names))):\n",
    "    name = all_names[idx]\n",
    "    t[idx]=find_matches(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_key in t:\n",
    "    \n",
    "    te_df = new_unified_person_df.iloc[list(t[temp_key])]\n",
    "\n",
    "    name_list = list(itertools.chain.from_iterable(te_df['name_list'].values))\n",
    "    id_list = list(itertools.chain.from_iterable(te_df['id_list'].values))\n",
    "    description_list = list(itertools.chain.from_iterable(te_df['description_list'].values))\n",
    "\n",
    "    new_unified_person_df.at[temp_key, 'name_list'] = name_list\n",
    "    new_unified_person_df.at[temp_key, 'id_list'] = id_list\n",
    "    new_unified_person_df.at[temp_key, 'description_list'] = description_list\n",
    "\n",
    "new_unified_person_df = new_unified_person_df.loc[t.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df[new_unified_person_df['name_list'].apply(lambda x: len(x)>=2)]['name_list'].sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unified person table\n",
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: find each person's wikidata entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "user_agent = 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'\n",
    "\n",
    "sparqlwd = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "sparqlwd.setReturnFormat(JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wiki_entity(name):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?item WHERE {\n",
    "        SERVICE wikibase:mwapi {\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search  \\'\"\"\"+name+\"\"\"\\';\n",
    "                            mwapi:language \"en\".\n",
    "            ?item wikibase:apiOutputItem mwapi:item.\n",
    "            ?num wikibase:apiOrdinal true.\n",
    "        }\n",
    "        ?item wdt:P31 wd:Q5\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {name}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_name_list(row):\n",
    "\n",
    "    name_list = row['name_list']\n",
    "\n",
    "    wiki_tag = set()\n",
    "\n",
    "    for name in name_list:\n",
    "        res = find_wiki_entity(name)\n",
    "\n",
    "        for binding in res['results']['bindings']:\n",
    "            wiki_tag.add(binding['item']['value'])\n",
    "\n",
    "    return list(wiki_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 53/13317 [00:54<4:09:10,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: George C. Denny\n",
      "error message: Remote end closed connection without response\n",
      "name: Jr. George C. Denny\n",
      "error message: <urlopen error [Errno 54] Connection reset by peer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 11537/13317 [1:26:52<09:17,  3.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: David Loving\n",
      "error message: Remote end closed connection without response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 11538/13317 [1:26:53<10:08,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Italo A. Luder\n",
      "error message: <urlopen error [Errno 54] Connection reset by peer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13317/13317 [1:36:08<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "new_unified_person_df = pd.read_parquet('tables/tables_52_88/new_unified_person_df.parquet')\n",
    "wiki_col = new_unified_person_df.progress_apply(lambda x: process_name_list(x),axis=1)\n",
    "new_unified_person_df['wiki_col'] = wiki_col\n",
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df_wikicol.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 5: reduce multiple candidate wikidata entities to single using sbert for each person, if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for using sbert for deciding among wikidata entries\n",
    "def get_entity_descp(Q):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?descp\n",
    "        WHERE \n",
    "        {\n",
    "        wd:\"\"\"+Q+\"\"\" schema:description ?descp.\n",
    "        FILTER ( lang(?descp) = \"en\" )\n",
    "        }\"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {Q}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_candidate_entities(row):\n",
    "\n",
    "    q_list = row['wiki_col']\n",
    "    \n",
    "    wiki_descp = []\n",
    "\n",
    "    for q in q_list:\n",
    "        \n",
    "        res = get_entity_descp(q.split('/')[-1])\n",
    "        \n",
    "        if len(res['results']['bindings'])==0:\n",
    "            wiki_descp.append('')\n",
    "        else:      \n",
    "            for binding in res['results']['bindings']:\n",
    "\n",
    "                wiki_descp.append(binding['descp']['value'])\n",
    "\n",
    "    return wiki_descp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wiki_col(row):\n",
    "\n",
    "    wiki_col = row['wiki_col']\n",
    "    \n",
    "    if len(wiki_col)==0:\n",
    "        return None\n",
    "\n",
    "    elif len(wiki_col)==1:\n",
    "        return wiki_col[0]\n",
    "\n",
    "    else:\n",
    "        desc_list = row['description_list']\n",
    "        frus_embedding = np.mean(model.encode(desc_list), axis=0)\n",
    "\n",
    "        wiki_descs = process_candidate_entities(row)\n",
    "        wiki_embeddings = model.encode(wiki_descs)\n",
    "\n",
    "        cos_sim = util.cos_sim(frus_embedding, wiki_embeddings)\n",
    "\n",
    "        selected_idx = np.argmax(cos_sim,axis=1)[0]\n",
    "        \n",
    "        return row[\"wiki_col\"][selected_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2413/13317 [27:23<1:20:16,  2.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q2707822\n",
      "error message: [Errno 54] Connection reset by peer\n",
      "name: Q96246951\n",
      "error message: <urlopen error [Errno 54] Connection reset by peer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 5965/13317 [49:31<08:09, 15.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q959580\n",
      "error message: HTTP Error 504: Gateway Timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6153/13317 [51:21<36:37,  3.26it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q76165936\n",
      "error message: Remote end closed connection without response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 6536/13317 [54:31<07:14, 15.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q105692850\n",
      "error message: HTTP Error 504: Gateway Timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 6537/13317 [55:22<1:42:11,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q110022064\n",
      "error message: <urlopen error [Errno 54] Connection reset by peer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6688/13317 [55:34<09:45, 11.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q99693179\n",
      "error message: Remote end closed connection without response\n",
      "name: Q102138869\n",
      "error message: <urlopen error [Errno 54] Connection reset by peer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 9741/13317 [1:08:15<12:59,  4.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q18151892\n",
      "error message: [Errno 54] Connection reset by peer\n",
      "name: Q213550\n",
      "error message: <urlopen error [Errno 54] Connection reset by peer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 9747/13317 [1:08:41<1:08:58,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q96207190\n",
      "error message: HTTP Error 504: Gateway Timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 9841/13317 [1:12:30<2:15:00,  2.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q24060518\n",
      "error message: HTTP Error 504: Gateway Timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 10258/13317 [1:13:59<17:59,  2.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q55769789\n",
      "error message: HTTP Error 504: Gateway Timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 10645/13317 [1:17:02<09:32,  4.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q2958645\n",
      "error message: HTTP Error 504: Gateway Timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 11904/13317 [1:22:32<03:27,  6.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Q55979684\n",
      "error message: HTTP Error 504: Gateway Timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13317/13317 [1:30:15<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "new_unified_person_df_wikicol = pd.read_parquet('tables/tables_52_88/new_unified_person_df_wikicol.parquet')\n",
    "selected_wiki_entity = new_unified_person_df.progress_apply(lambda x: process_wiki_col(x),axis=1)\n",
    "\n",
    "new_unified_person_df['selected_wiki_entity'] = selected_wiki_entity\n",
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df_sbert.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 6: reduce names with exactly same wikidata entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df = pd.read_parquet('tables/tables_52_88/new_unified_person_df_sbert.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "\n",
    "for idx, key in new_unified_person_df.iterrows():\n",
    "\n",
    "    ent = key['selected_wiki_entity']\n",
    "\n",
    "    if not ent:\n",
    "        t[idx]=set([idx])\n",
    "    else:\n",
    "        t[idx]=set(new_unified_person_df[new_unified_person_df['selected_wiki_entity']==ent].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 241 keys.\n",
      "---\n",
      "removing 0 keys.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "\n",
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_key in t:\n",
    "    \n",
    "    te_df = new_unified_person_df.loc[list(t[temp_key])]\n",
    "\n",
    "    name_list = list(itertools.chain.from_iterable(te_df['name_list'].values))\n",
    "    id_list = list(itertools.chain.from_iterable(te_df['id_list'].values))\n",
    "    description_list = list(itertools.chain.from_iterable(te_df['description_list'].values))\n",
    "\n",
    "    new_unified_person_df.at[temp_key, 'name_list'] = name_list\n",
    "    new_unified_person_df.at[temp_key, 'id_list'] = id_list\n",
    "    new_unified_person_df.at[temp_key, 'description_list'] = description_list\n",
    "\n",
    "new_unified_person_df = new_unified_person_df.loc[t.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df_final.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
