{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import glob \n",
    "import math\n",
    "import itertools\n",
    "import jellyfish\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "ns = {'xml': 'http://www.w3.org/XML/1998/namespace',\n",
    "      'dflt': 'http://www.tei-c.org/ns/1.0',\n",
    "      'frus':'http://history.state.gov/frus/ns/1.0',\n",
    "      'xi':'http://www.w3.org/2001/XInclude'\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person(item, file):\n",
    "    volume = file[8:-4]\n",
    "\n",
    "    persName_item = item.find('.//dflt:persName[@xml:id]', ns)\n",
    "\n",
    "    if persName_item is not None:\n",
    "\n",
    "        persName_text = \"\".join(persName_item.itertext())\n",
    "        person_id = persName_item.attrib['{http://www.w3.org/XML/1998/namespace}id']\n",
    "\n",
    "        all_text = \"\".join(item.itertext())\n",
    "        end_idx = all_text.find(persName_text) + len(persName_text+',')\n",
    "        person_descp = \" \".join(all_text[end_idx:].split())\n",
    "\n",
    "        person_name = \" \".join(re.sub(',','',\" \".join(persName_text.split(', ')[::-1])).split())\n",
    "\n",
    "        person_id = volume + '_' + person_id\n",
    "\n",
    "        global person_df\n",
    "        person_df = pd.concat((person_df, pd.DataFrame({'id':[person_id],\n",
    "                                                    'name':[person_name],\n",
    "                                                    'description':[person_descp]})),ignore_index=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No person annotation in volumes/frus1952-54v01p2.xml.\n",
      "No person annotation in volumes/frus1952-54v01p1.xml.\n",
      "No person annotation in volumes/frus1977-80v09.xml.\n",
      "No person annotation in volumes/frus1952-54v12p1.xml.\n",
      "No person annotation in volumes/frus1952-54v12p2.xml.\n"
     ]
    }
   ],
   "source": [
    "start_year, end_year = 1952, 1988\n",
    "\n",
    "person_df = pd.DataFrame(columns=['id','name','description'])\n",
    "\n",
    "for file in glob.glob('volumes/frus*'):\n",
    "    file_start_year = int(file[12:16])\n",
    "    \n",
    "    if file_start_year >= start_year and file_start_year<=end_year:\n",
    "\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        persons_section = root.find(\"./dflt:text/dflt:front//dflt:div[@xml:id='persons']\", ns)\n",
    "        \n",
    "        if persons_section:\n",
    "            for item in persons_section.findall('.//dflt:item/dflt:hi/dflt:persName[@xml:id]/../..', ns):\n",
    "                extract_person(item,file)\n",
    "            for item in persons_section.findall('.//dflt:item/dflt:persName[@xml:id]/..', ns):\n",
    "                extract_person(item,file)\n",
    "        else:\n",
    "            print(f'No person annotation in {file}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: reduce exactly matched names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux(row):\n",
    "    global unified_person_dict\n",
    "\n",
    "    if row['name'] in unified_person_dict:\n",
    "      \n",
    "      temp_dict = unified_person_dict[row['name']]\n",
    "\n",
    "      temp_dict['id_list'].append(row['id'])\n",
    "      temp_dict['description_list'].append(row['description'])\n",
    "    \n",
    "    else:\n",
    "      unified_person_dict[row['name']]= {'id_list':[row['id']],\n",
    "                                        'description_list':[row['description']]}\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "48358    None\n",
       "48359    None\n",
       "48360    None\n",
       "48361    None\n",
       "48362    None\n",
       "Length: 48363, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_df.apply(lambda x:aux(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_df = pd.DataFrame.from_dict(unified_person_dict,orient='index').reset_index(drop=False)\n",
    "unified_person_df.rename(columns={'index':'name'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: reduce names with exactly same words but different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_df['name_set'] = unified_person_df.name.apply(lambda x: \" \".join(sorted(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_dict = {}\n",
    "\n",
    "def aux2(row):\n",
    "    global new_unified_person_dict\n",
    "\n",
    "    if row['name_set'] in new_unified_person_dict:\n",
    "      \n",
    "        temp_dict = new_unified_person_dict[row['name_set']]\n",
    "\n",
    "        temp_dict['name_list'].append(row['name'])\n",
    "        temp_dict['id_list'] += row['id_list']\n",
    "        temp_dict['description_list'] += row['description_list']\n",
    "    \n",
    "    else:\n",
    "        new_unified_person_dict[row['name_set']]= {'name_list':[row['name']],\n",
    "                                                    'id_list':row['id_list'],\n",
    "                                                    'description_list':row['description_list']}\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_person_df.apply(lambda x:aux2(x), axis=1)\n",
    "\n",
    "new_unified_person_df = pd.DataFrame.from_dict(new_unified_person_dict,orient='index').reset_index(drop=False)\n",
    "new_unified_person_df.rename(columns={'index':'name_set'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_set</th>\n",
       "      <th>name_list</th>\n",
       "      <th>id_list</th>\n",
       "      <th>description_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acheson Dean</td>\n",
       "      <td>[Dean Acheson, Acheson Dean]</td>\n",
       "      <td>[frus1964-68v03_p_AD1, frus1969-76v38p1_p_AD_1...</td>\n",
       "      <td>[Secretary of State from 1949 until 1953, Secr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Boggs Hale</td>\n",
       "      <td>[Hale Boggs, Boggs Hale]</td>\n",
       "      <td>[frus1964-68v03_p_BH1, frus1964-68v02_p_BH1, f...</td>\n",
       "      <td>[Democratic Representative from Louisiana, Dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Brown Harold</td>\n",
       "      <td>[Harold Brown, Brown Harold]</td>\n",
       "      <td>[frus1964-68v03_p_BH2, frus1964-68v02_p_BH2, f...</td>\n",
       "      <td>[Director, Defense Research and Engineering, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bui Diem</td>\n",
       "      <td>[Bui Diem, Diem Bui]</td>\n",
       "      <td>[frus1964-68v03_p_BD1, frus1969-76v14_p_BD5, f...</td>\n",
       "      <td>[Vietnamese Chief of Staff in the Quat governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bundy McGeorge</td>\n",
       "      <td>[Bundy McGeorge, McGeorge Bundy]</td>\n",
       "      <td>[frus1964-68v03_p_BMG1, frus1961-63v11_p_BMG2,...</td>\n",
       "      <td>[President’s Special Assistant for National Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16876</th>\n",
       "      <td>Aidit Dipa Nusantara</td>\n",
       "      <td>[Aidit Dipa Nusantara, Dipa Nusantara Aidit]</td>\n",
       "      <td>[frus1961-63v23_p_ADN1, frus1964-68v26_p_ADN1]</td>\n",
       "      <td>[leader of the PKI (Partai Komunis Indonesia/I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>Cater Douglass Jr. S.</td>\n",
       "      <td>[S. Douglass Jr. Cater, Jr. S. Douglass Cater]</td>\n",
       "      <td>[frus1964-68v31_p_CSDJ1, frus1964-68v32_p_CSDJ1]</td>\n",
       "      <td>[Special Assistant to the President July 1965–...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17040</th>\n",
       "      <td>J. Jr. Miguel Moreno</td>\n",
       "      <td>[Jr. Miguel J. Moreno, Miguel J. Jr. Moreno]</td>\n",
       "      <td>[frus1964-68v31_p_MMJJ1, frus1958-60v05_p_MMJJ1]</td>\n",
       "      <td>[Panamanian Representative to the Council of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17198</th>\n",
       "      <td>Henderson Loy</td>\n",
       "      <td>[Henderson Loy, Loy Henderson]</td>\n",
       "      <td>[frus1958-60v11_p_HLW1, frus1958-60v05_p_HL1]</td>\n",
       "      <td>[Deputy Under Secretary of State for Administr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17520</th>\n",
       "      <td>Amory Derick Heathcoat</td>\n",
       "      <td>[Derick Heathcoat Amory, Heathcoat Amory Derick]</td>\n",
       "      <td>[frus1958-60v04_p_ADH1, frus1958-60v04_p_HAD1]</td>\n",
       "      <td>[see Heathcoat Amory, Derick, British Chancell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1637 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name_set  \\\n",
       "0                Acheson Dean   \n",
       "9                  Boggs Hale   \n",
       "14               Brown Harold   \n",
       "17                   Bui Diem   \n",
       "20             Bundy McGeorge   \n",
       "...                       ...   \n",
       "16876    Aidit Dipa Nusantara   \n",
       "16996   Cater Douglass Jr. S.   \n",
       "17040    J. Jr. Miguel Moreno   \n",
       "17198           Henderson Loy   \n",
       "17520  Amory Derick Heathcoat   \n",
       "\n",
       "                                              name_list  \\\n",
       "0                          [Dean Acheson, Acheson Dean]   \n",
       "9                              [Hale Boggs, Boggs Hale]   \n",
       "14                         [Harold Brown, Brown Harold]   \n",
       "17                                 [Bui Diem, Diem Bui]   \n",
       "20                     [Bundy McGeorge, McGeorge Bundy]   \n",
       "...                                                 ...   \n",
       "16876      [Aidit Dipa Nusantara, Dipa Nusantara Aidit]   \n",
       "16996    [S. Douglass Jr. Cater, Jr. S. Douglass Cater]   \n",
       "17040      [Jr. Miguel J. Moreno, Miguel J. Jr. Moreno]   \n",
       "17198                    [Henderson Loy, Loy Henderson]   \n",
       "17520  [Derick Heathcoat Amory, Heathcoat Amory Derick]   \n",
       "\n",
       "                                                 id_list  \\\n",
       "0      [frus1964-68v03_p_AD1, frus1969-76v38p1_p_AD_1...   \n",
       "9      [frus1964-68v03_p_BH1, frus1964-68v02_p_BH1, f...   \n",
       "14     [frus1964-68v03_p_BH2, frus1964-68v02_p_BH2, f...   \n",
       "17     [frus1964-68v03_p_BD1, frus1969-76v14_p_BD5, f...   \n",
       "20     [frus1964-68v03_p_BMG1, frus1961-63v11_p_BMG2,...   \n",
       "...                                                  ...   \n",
       "16876     [frus1961-63v23_p_ADN1, frus1964-68v26_p_ADN1]   \n",
       "16996   [frus1964-68v31_p_CSDJ1, frus1964-68v32_p_CSDJ1]   \n",
       "17040   [frus1964-68v31_p_MMJJ1, frus1958-60v05_p_MMJJ1]   \n",
       "17198      [frus1958-60v11_p_HLW1, frus1958-60v05_p_HL1]   \n",
       "17520     [frus1958-60v04_p_ADH1, frus1958-60v04_p_HAD1]   \n",
       "\n",
       "                                        description_list  \n",
       "0      [Secretary of State from 1949 until 1953, Secr...  \n",
       "9      [Democratic Representative from Louisiana, Dem...  \n",
       "14     [Director, Defense Research and Engineering, D...  \n",
       "17     [Vietnamese Chief of Staff in the Quat governm...  \n",
       "20     [President’s Special Assistant for National Se...  \n",
       "...                                                  ...  \n",
       "16876  [leader of the PKI (Partai Komunis Indonesia/I...  \n",
       "16996  [Special Assistant to the President July 1965–...  \n",
       "17040  [Panamanian Representative to the Council of t...  \n",
       "17198  [Deputy Under Secretary of State for Administr...  \n",
       "17520  [see Heathcoat Amory, Derick, British Chancell...  \n",
       "\n",
       "[1637 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for observation\n",
    "new_unified_person_df[new_unified_person_df['name_list'].apply(lambda x: len(x)==2)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: find and reduce near-duplicate names + obvious misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one: (match len>=2 and each word len>=3), edit distance based matching\n",
    "# step two: find misspelling matches with edit distance of 1 e.g. for Ziegler vs Zeigler\n",
    "\n",
    "# caution!!!\n",
    "# Eliot Jr. L. Theodore, and D. Dwight Eisenhower\n",
    "# Georges Guay R. vs George Guay R.\n",
    "# Abrams Creighton General Major W.\n",
    "# Aharon General Major Yariv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = new_unified_person_df['name_set'].values\n",
    "\n",
    "def compute_sim(s1,func,s2):\n",
    "    return func(s1,s2)\n",
    "\n",
    "def compute_exact_word_overlap(s1,s2):\n",
    "    l1 = set([x for x in list(set(tokenizer.tokenize(s1))) if len(x)>=3])\n",
    "    l2 = set([x for x in list(set(tokenizer.tokenize(s2))) if len(x)>=3])\n",
    "\n",
    "    return len(l1.intersection(l2))\n",
    "\n",
    "def find_matches(s2):\n",
    "\n",
    "    spiro_dist_df = pd.DataFrame({'name_set':all_names,\n",
    "                                'overlap_cnt':[compute_exact_word_overlap(x,s2) for x in all_names],\n",
    "                                'dam_lev_dist':[compute_sim(x, jellyfish.damerau_levenshtein_distance,s2) for x in all_names],\n",
    "                                'jaro_sim':[compute_sim(x, jellyfish.jaro_winkler_similarity,s2) for x in all_names]})\n",
    "    \n",
    "    # addition to original matching criteria\n",
    "    misspelling_idx = set(spiro_dist_df[(spiro_dist_df['dam_lev_dist'] <=1)].index.values)\n",
    "\n",
    "    spiro_dist_df = spiro_dist_df[spiro_dist_df['overlap_cnt']>=2]\n",
    "    match_idx = set(spiro_dist_df[(spiro_dist_df['jaro_sim'] >= 0.9) | (spiro_dist_df['dam_lev_dist'] <=5)].index.values)\n",
    "\n",
    "    return match_idx.union(misspelling_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17633/17633 [1:02:17<00:00,  4.72it/s]\n"
     ]
    }
   ],
   "source": [
    "t = {}\n",
    "for idx in tqdm(range(len(all_names))):\n",
    "    name = all_names[idx]\n",
    "    t[idx]=find_matches(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 3821 keys.\n",
      "---\n",
      "removing 482 keys.\n",
      "---\n",
      "removing 12 keys.\n",
      "---\n",
      "removing 1 keys.\n",
      "---\n",
      "removing 0 keys.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_key in t:\n",
    "    \n",
    "    te_df = new_unified_person_df.iloc[list(t[temp_key])]\n",
    "\n",
    "    name_list = list(itertools.chain.from_iterable(te_df['name_list'].values))\n",
    "    id_list = list(itertools.chain.from_iterable(te_df['id_list'].values))\n",
    "    description_list = list(itertools.chain.from_iterable(te_df['description_list'].values))\n",
    "\n",
    "    new_unified_person_df.at[temp_key, 'name_list'] = name_list\n",
    "    new_unified_person_df.at[temp_key, 'id_list'] = id_list\n",
    "    new_unified_person_df.at[temp_key, 'description_list'] = description_list\n",
    "\n",
    "new_unified_person_df = new_unified_person_df.loc[t.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Betencourt Rómulo', 'Escobar Betancourt Rómulo', 'Betancourt Rómulo', 'Rómulo Betancourt', 'Betancourt Romulo']),\n",
       "       list(['Morton I. “Mort” Abramowitz', 'Morton I. Abramovitz', 'Morton I. Abramowitz', 'Morton Abramowitz', 'Abramowitz Morton “Mort”']),\n",
       "       list(['Edward G. Jr. Biester', 'Edward George Jr. Biester']),\n",
       "       list(['Spencer M. King', 'Spencer King']),\n",
       "       list(['Robert Lawrence Coughlin', 'Coughlin Robert Lawrence']),\n",
       "       list(['Manzur Qadir', 'Mansur Qadir']),\n",
       "       list(['Alexander Butterfield', 'Butterfield Alexander', 'Alexander P. Butterfield', 'Butterfield Alexander P.']),\n",
       "       list(['Stefan Cardinal Wyszynski', 'Cardinal Stefan Wyszynski']),\n",
       "       list(['John L. McClellan', 'McClellan John', 'John Little McClellan']),\n",
       "       list(['Charles Z Wick', 'Charles Z. Wick'])], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_unified_person_df[new_unified_person_df['name_list'].apply(lambda x: len(x)>=2)]['name_list'].sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save unified person table\n",
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: find each person's wikidata entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "user_agent = 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'\n",
    "\n",
    "sparqlwd = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "sparqlwd.setReturnFormat(JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wiki_entity(name):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?item WHERE {\n",
    "        SERVICE wikibase:mwapi {\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search  \\'\"\"\"+name+\"\"\"\\';\n",
    "                            mwapi:language \"en\".\n",
    "            ?item wikibase:apiOutputItem mwapi:item.\n",
    "            ?num wikibase:apiOrdinal true.\n",
    "        }\n",
    "        ?item wdt:P31 wd:Q5\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {name}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_name_list(row):\n",
    "\n",
    "    name_list = row['name_list']\n",
    "\n",
    "    wiki_tag = set()\n",
    "\n",
    "    for name in name_list:\n",
    "        res = find_wiki_entity(name)\n",
    "\n",
    "        for binding in res['results']['bindings']:\n",
    "            wiki_tag.add(binding['item']['value'])\n",
    "\n",
    "    return list(wiki_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df = pd.read_parquet('tables/tables_52_88/new_unified_person_df.parquet')\n",
    "wiki_col = new_unified_person_df.progress_apply(lambda x: process_name_list(x),axis=1)\n",
    "new_unified_person_df['wiki_col'] = wiki_col\n",
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df_wikicol.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 5: reduce multiple candidate wikidata entities to single using sbert for each person, if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df_wikicol = pd.read_parquet('tables/tables_52_88/new_unified_person_df_wikicol.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df_wikicol[new_unified_person_df_wikicol['wiki_col'].apply(lambda x: True if len(x)>1 else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for using sbert for deciding among wikidata entries\n",
    "def get_entity_descp(Q):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?descp\n",
    "        WHERE \n",
    "        {\n",
    "        wd:\"\"\"+Q+\"\"\" schema:description ?descp.\n",
    "        FILTER ( lang(?descp) = \"en\" )\n",
    "        }\"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {Q}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_candidate_entities(row):\n",
    "\n",
    "    q_list = row['wiki_col']\n",
    "    \n",
    "    wiki_descp = []\n",
    "\n",
    "    for q in q_list:\n",
    "        \n",
    "        res = get_entity_descp(q.split('/')[-1])\n",
    "        \n",
    "        if len(res['results']['bindings'])==0:\n",
    "            wiki_descp.append('')\n",
    "        else:      \n",
    "            for binding in res['results']['bindings']:\n",
    "\n",
    "                wiki_descp.append(binding['descp']['value'])\n",
    "\n",
    "    return wiki_descp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wiki_col(row):\n",
    "\n",
    "    wiki_col = row['wiki_col']\n",
    "    \n",
    "    if len(wiki_col)==0:\n",
    "        return None\n",
    "\n",
    "    elif len(wiki_col)==1:\n",
    "        return wiki_col[0]\n",
    "\n",
    "    else:\n",
    "        desc_list = row['description_list']\n",
    "        frus_embedding = np.mean(model.encode(desc_list), axis=0)\n",
    "\n",
    "        wiki_descs = process_candidate_entities(row)\n",
    "        wiki_embeddings = model.encode(wiki_descs)\n",
    "\n",
    "        cos_sim = util.cos_sim(frus_embedding, wiki_embeddings)\n",
    "\n",
    "        selected_idx = np.argmax(cos_sim,axis=1)[0]\n",
    "        \n",
    "        return row[\"wiki_col\"][selected_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_wiki_entity = new_unified_person_df.progress_apply(lambda x: process_wiki_col(x),axis=1)\n",
    "\n",
    "new_unified_person_df['selected_wiki_entity'] = selected_wiki_entity\n",
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df_sbert.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 6: reduce names with exactly same wikidata entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df = pd.read_parquet('tables/tables_52_88/new_unified_person_df_sbert.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "\n",
    "for idx, key in new_unified_person_df.iterrows():\n",
    "\n",
    "    ent = key['selected_wiki_entity']\n",
    "\n",
    "    if not ent:\n",
    "        t[idx]=set([idx])\n",
    "    else:\n",
    "        t[idx]=set(new_unified_person_df[new_unified_person_df['selected_wiki_entity']==ent].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "\n",
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_key in t:\n",
    "    \n",
    "    te_df = new_unified_person_df.loc[list(t[temp_key])]\n",
    "\n",
    "    name_list = list(itertools.chain.from_iterable(te_df['name_list'].values))\n",
    "    id_list = list(itertools.chain.from_iterable(te_df['id_list'].values))\n",
    "    description_list = list(itertools.chain.from_iterable(te_df['description_list'].values))\n",
    "\n",
    "    new_unified_person_df.at[temp_key, 'name_list'] = name_list\n",
    "    new_unified_person_df.at[temp_key, 'id_list'] = id_list\n",
    "    new_unified_person_df.at[temp_key, 'description_list'] = description_list\n",
    "\n",
    "new_unified_person_df = new_unified_person_df.loc[t.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df.to_parquet('tables/tables_52_88/new_unified_person_df_final.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
