{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'\n",
    "\n",
    "sparqlwd = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "sparqlwd.setReturnFormat(JSON)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## person matching (move to person_unify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_wiki_entity(name):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?item WHERE {\n",
    "        SERVICE wikibase:mwapi {\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search  \\'\"\"\"+name+\"\"\"\\';\n",
    "                            mwapi:language \"en\".\n",
    "            ?item wikibase:apiOutputItem mwapi:item.\n",
    "            ?num wikibase:apiOrdinal true.\n",
    "        }\n",
    "        ?item wdt:P31 wd:Q5\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {name}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_name_list(row):\n",
    "\n",
    "    name_list = row['name_list']\n",
    "\n",
    "    wiki_tag = set()\n",
    "\n",
    "    for name in name_list:\n",
    "        res = find_wiki_entity(name)\n",
    "\n",
    "        for binding in res['results']['bindings']:\n",
    "            wiki_tag.add(binding['item']['value'])\n",
    "\n",
    "    return list(wiki_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df = pd.read_parquet('tables/new_unified_person_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_col = new_unified_person_df.progress_apply(lambda x: process_name_list(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df['wiki_col'] = wiki_col\n",
    "new_unified_person_df.to_parquet('tables/new_unified_person_df_wikicol.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df_wikicol = pd.read_parquet('tables/new_unified_person_df_wikicol.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df_wikicol[new_unified_person_df_wikicol['wiki_col'].apply(lambda x: True if len(x)>1 else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for using sbert for deciding among wikidata entries\n",
    "def get_entity_descp(Q):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?descp\n",
    "        WHERE \n",
    "        {\n",
    "        wd:\"\"\"+Q+\"\"\" schema:description ?descp.\n",
    "        FILTER ( lang(?descp) = \"en\" )\n",
    "        }\"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {Q}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_candidate_entities(row):\n",
    "\n",
    "    q_list = row['wiki_col']\n",
    "    \n",
    "    wiki_descp = []\n",
    "\n",
    "    for q in q_list:\n",
    "        \n",
    "        res = get_entity_descp(q.split('/')[-1])\n",
    "        \n",
    "        if len(res['results']['bindings'])==0:\n",
    "            wiki_descp.append('')\n",
    "        else:      \n",
    "            for binding in res['results']['bindings']:\n",
    "\n",
    "                wiki_descp.append(binding['descp']['value'])\n",
    "\n",
    "    return wiki_descp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wiki_col(row):\n",
    "\n",
    "    wiki_col = row['wiki_col']\n",
    "    \n",
    "    if len(wiki_col)==0:\n",
    "        return None\n",
    "\n",
    "    elif len(wiki_col)==1:\n",
    "        return wiki_col[0]\n",
    "\n",
    "    else:\n",
    "        desc_list = row['description_list']\n",
    "        frus_embedding = np.mean(model.encode(desc_list), axis=0)\n",
    "\n",
    "        wiki_descs = process_candidate_entities(row)\n",
    "        wiki_embeddings = model.encode(wiki_descs)\n",
    "\n",
    "        cos_sim = util.cos_sim(frus_embedding, wiki_embeddings)\n",
    "\n",
    "        selected_idx = np.argmax(cos_sim,axis=1)[0]\n",
    "        \n",
    "        return row[\"wiki_col\"][selected_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_wiki_entity = new_unified_person_df.progress_apply(lambda x: process_wiki_col(x),axis=1)\n",
    "\n",
    "new_unified_person_df['selected_wiki_entity'] = selected_wiki_entity\n",
    "new_unified_person_df.to_parquet('tables/new_unified_person_df_sbert.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce names with exactly same wikidata entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df = pd.read_parquet('tables/new_unified_person_df_sbert.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {}\n",
    "\n",
    "for idx, key in new_unified_person_df.iterrows():\n",
    "\n",
    "    ent = key['selected_wiki_entity']\n",
    "\n",
    "    if not ent:\n",
    "        t[idx]=set([idx])\n",
    "    else:\n",
    "        t[idx]=set(new_unified_person_df[new_unified_person_df['selected_wiki_entity']==ent].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "\n",
    "scratch_t = copy.deepcopy(t)\n",
    "changed_flag = True\n",
    "\n",
    "while changed_flag:\n",
    "\n",
    "    changed_flag = False\n",
    "\n",
    "    for key in t:\n",
    "        \n",
    "        for matched_idx in t[key]:\n",
    "\n",
    "            if key != matched_idx:\n",
    "                if scratch_t.get(key, None) and scratch_t.get(matched_idx, None):\n",
    "                    changed_flag = True\n",
    "                    t[key] = t[key].union(t[matched_idx])\n",
    "                    scratch_t.pop(matched_idx, None)\n",
    "        \n",
    "    unwanted = set(t.keys()) - set(scratch_t.keys())\n",
    "    print(f'removing {len(unwanted)} keys.')\n",
    "    for unwanted_key in unwanted: del t[unwanted_key]\n",
    "    scratch_t = copy.deepcopy(t)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_key in t:\n",
    "    \n",
    "    te_df = new_unified_person_df.loc[list(t[temp_key])]\n",
    "\n",
    "    name_list = list(itertools.chain.from_iterable(te_df['name_list'].values))\n",
    "    id_list = list(itertools.chain.from_iterable(te_df['id_list'].values))\n",
    "    description_list = list(itertools.chain.from_iterable(te_df['description_list'].values))\n",
    "\n",
    "    new_unified_person_df.at[temp_key, 'name_list'] = name_list\n",
    "    new_unified_person_df.at[temp_key, 'id_list'] = id_list\n",
    "    new_unified_person_df.at[temp_key, 'description_list'] = description_list\n",
    "\n",
    "new_unified_person_df = new_unified_person_df.loc[t.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df.to_parquet('tables/new_unified_person_df_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting extra info from wikidata (make this separate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df = pd.read_parquet('tables/new_unified_person_df_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" wdt:P21 ?item;\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n",
    "\n",
    "religion_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" wdt:P140 ?item.\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n",
    "\n",
    "educated_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" wdt:P69 ?item.\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n",
    "\n",
    "occupation_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" wdt:P106 ?item.\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n",
    "\n",
    "citizenship_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel ?startyearLabel ?endyearLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" p:P27 ?statement1.\n",
    "?statement1 ps:P27 ?item.\n",
    "OPTIONAL{?statement1 pq:P580 ?startyear.}\n",
    "OPTIONAL{?statement1 pq:P582 ?endyear.}\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n",
    "\n",
    "party_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel ?startyearLabel ?endyearLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" p:P102 ?statement1.\n",
    "?statement1 ps:P102 ?item.\n",
    "OPTIONAL{?statement1 pq:P580 ?startyear.}\n",
    "OPTIONAL{?statement1 pq:P582 ?endyear.}\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n",
    "\n",
    "memberof_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel ?startyearLabel ?endyearLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" p:P463 ?statement1.\n",
    "?statement1 ps:P463 ?item.\n",
    "OPTIONAL{?statement1 pq:P580 ?startyear.}\n",
    "OPTIONAL{?statement1 pq:P582 ?endyear.}\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n",
    "\n",
    "positionheld_f=lambda Q:\"\"\"\n",
    "SELECT ?item ?itemLabel ?startyearLabel ?endyearLabel\n",
    "WHERE \n",
    "{\n",
    "wd:\"\"\"+Q+\"\"\" p:P39 ?statement1.\n",
    "?statement1 ps:P39 ?item.\n",
    "OPTIONAL{?statement1 pq:P580 ?startyear.}\n",
    "OPTIONAL{?statement1 pq:P582 ?endyear.}\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_dict={'gender':gender_f,\n",
    "               'religion':religion_f,\n",
    "               'educated':educated_f,\n",
    "               'occupation':occupation_f,\n",
    "               'positionheld':positionheld_f,\n",
    "               'citizenship':citizenship_f,\n",
    "               'memberof':memberof_f,\n",
    "               'party':party_f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(type,entity):\n",
    "\n",
    "    try:\n",
    "        sparqlwd.setQuery(function_dict[type](entity))\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {entity}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_query(row,type):\n",
    "    \n",
    "    entity = row['selected_wiki_entity']\n",
    "\n",
    "    retrieved = []\n",
    "\n",
    "    if entity:\n",
    "\n",
    "        entity = entity.split('/')[-1]\n",
    "\n",
    "        res = execute_query(type,entity)\n",
    "\n",
    "        for binding in res['results']['bindings']:\n",
    "            temp = []\n",
    "            temp.append(binding['item']['value'])\n",
    "            temp.append(binding['itemLabel']['value'])\n",
    "            if binding.get('startyearLabel',None):\n",
    "                temp.append(binding['startyearLabel']['value'])\n",
    "            if binding.get('endyearLabel',None):\n",
    "                temp.append(binding['endyearLabel']['value'])\n",
    "        \n",
    "            if len(temp)>0:\n",
    "                retrieved.append(temp)\n",
    "\n",
    "    if len(retrieved)>0:\n",
    "        return retrieved\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4690/4690 [09:27<00:00,  8.27it/s] \n",
      "100%|██████████| 4690/4690 [09:06<00:00,  8.58it/s]\n",
      "100%|██████████| 4690/4690 [09:33<00:00,  8.17it/s]\n",
      "100%|██████████| 4690/4690 [09:29<00:00,  8.24it/s]\n",
      "100%|██████████| 4690/4690 [09:29<00:00,  8.23it/s] \n",
      "100%|██████████| 4690/4690 [09:31<00:00,  8.20it/s]\n",
      "100%|██████████| 4690/4690 [09:14<00:00,  8.46it/s]\n"
     ]
    }
   ],
   "source": [
    "gender_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('gender',))\n",
    "religion_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('religion',))\n",
    "educated_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('educated',))\n",
    "occupation_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('occupation',))\n",
    "positionheld_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('positionheld',))\n",
    "citizenship_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('citizenship',))\n",
    "party_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('party',))\n",
    "memberof_series = new_unified_person_df.progress_apply(process_query,axis=1,args=('memberof',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to this!\n",
    "merged_extra_df = pd.DataFrame.from_dict({'gender':gender_series,\n",
    "                        'religion':religion_series,\n",
    "                        'educated_at':educated_series,\n",
    "                        'occupation':occupation_series,\n",
    "                        'position_held':positionheld_series,\n",
    "                        'citizenship':citizenship_series,\n",
    "                        'member_of':memberof_series,\n",
    "                        'political_party':party_series})\n",
    "\n",
    "merged_extra_df.to_parquet('tables/person_wikidata_extras.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unified_person_df['gender'] = list(map(lambda x:x[0][1] if x else None,gender_series))\n",
    "new_unified_person_df.to_parquet('tables/new_unified_person_df_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_series_map = {'religion':religion_series,\n",
    "                    'school':educated_series,\n",
    "                    'occupation':occupation_series,\n",
    "                    'role':positionheld_series,\n",
    "                    'citizenship':citizenship_series,\n",
    "                    'political_party':party_series}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes of extra information\n",
    "\n",
    "# series with NO start-end year information\n",
    "for series_name in ['religion', 'school', 'occupation']:\n",
    "\n",
    "    series = name_series_map[series_name]\n",
    "\n",
    "    temp_df = pd.concat([new_unified_person_df['name_set'],series],axis=1)\n",
    "    temp_df.rename(columns={0:'info_list'},inplace=True)\n",
    "\n",
    "    info_df = pd.DataFrame(columns=['name_set','info_name','info_tag'])\n",
    "\n",
    "    def aux(row):\n",
    "        global info_df\n",
    "\n",
    "        name_set = row['name_set']\n",
    "        info_list = row['info_list']\n",
    "\n",
    "        if not info_list:\n",
    "            info_df = pd.concat((info_df,pd.DataFrame({'name_set':[name_set],'info_name':[None],'info_tag':[None]})))\n",
    "        else:\n",
    "            for info in info_list:\n",
    "                info_df = pd.concat((info_df,pd.DataFrame({'name_set':[name_set],'info_name':[info[1]],'info_tag':[info[0]]})))\n",
    "        \n",
    "        return\n",
    "\n",
    "    temp_df.apply(lambda x: aux(x),axis=1)\n",
    "\n",
    "    info_df.dropna(thresh=2,inplace=True) # exclude persons with no info\n",
    "    info_df.to_parquet('tables/person_'+series_name+'_69_76.parquet')\n",
    "\n",
    "\n",
    "# series with start-end year information\n",
    "for series_name in ['role', 'citizenship', 'political_party']:\n",
    "\n",
    "    series = name_series_map[series_name]\n",
    "\n",
    "    temp_df = pd.concat([new_unified_person_df['name_set'],series],axis=1)\n",
    "    temp_df.rename(columns={0:'info_list'},inplace=True)\n",
    "\n",
    "    info_df = pd.DataFrame(columns=['name_set','info_name','info_tag','start_year','end_year'])\n",
    "\n",
    "    def aux(row):\n",
    "        global info_df\n",
    "\n",
    "        name_set = row['name_set']\n",
    "        info_list = row['info_list']\n",
    "\n",
    "        if not info_list:\n",
    "            info_df = pd.concat((info_df,pd.DataFrame({'name_set':[name_set],\n",
    "                                                        'info_name':[None],\n",
    "                                                        'info_tag':[None],\n",
    "                                                        'start_year':[None],\n",
    "                                                        'end_year':[None]})))\n",
    "        else:\n",
    "            for info in info_list:\n",
    "                info_df = pd.concat((info_df,pd.DataFrame({'name_set':[name_set],\n",
    "                                                            'info_name':[info[1]],\n",
    "                                                            'info_tag':[info[0]],\n",
    "                                                            'start_year':[info[2] if len(info)>2 else None],\n",
    "                                                            'end_year':[info[3] if len(info)>3 else None]})))\n",
    "        \n",
    "        return\n",
    "\n",
    "    temp_df.apply(lambda x: aux(x),axis=1)\n",
    "\n",
    "    info_df.dropna(thresh=2,inplace=True) # exclude persons with no info\n",
    "    info_df.to_parquet('tables/person_'+series_name+'_69_76.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_tag(Q):\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT ?country ?countryLabel\n",
    "        WHERE \n",
    "        {\n",
    "        wd:\"\"\"+Q+\"\"\" wdt:P17 ?country.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "        }\"\"\"\n",
    "        \n",
    "        sparqlwd.setQuery(query)\n",
    "\n",
    "        return sparqlwd.query().convert()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'name: {Q}')\n",
    "        print(f'error message: {e}')\n",
    "        return {'head': {'vars': ['item']}, 'results': {'bindings': []}}\n",
    "\n",
    "\n",
    "def process_entities(entity):\n",
    "\n",
    "    res = get_country_tag(entity.split('/')[-1])\n",
    "    \n",
    "    if len(res['results']['bindings'])==0:\n",
    "        return ''\n",
    "    else: \n",
    "        # not checking multiple countries since meaningless\n",
    "        binding = res['results']['bindings'][0]\n",
    "\n",
    "        return binding['countryLabel']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_country_info(df):\n",
    "    party_tag_list = pd.unique(df['info_tag'])\n",
    "    country_col = list(map(lambda x: process_entities(x),party_tag_list))\n",
    "    party_tag_country_dict = dict(zip(party_tag_list,country_col))\n",
    "    return df['info_tag'].apply(lambda x: party_tag_country_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_party_df = pd.read_parquet('tables/person_political_party_69_76.parquet')\n",
    "person_party_df['country'] = add_country_info(person_party_df)\n",
    "person_party_df.to_parquet('tables/person_political_party_69_76.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_school_df = pd.read_parquet('tables/person_school_69_76.parquet')\n",
    "person_school_df['country'] = add_country_info(person_school_df)\n",
    "person_school_df.to_parquet('tables/person_school_69_76.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
